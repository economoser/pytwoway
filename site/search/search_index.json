{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to PyTwoWay","title":"Home"},{"location":"#welcome-to-pytwoway","text":"","title":"Welcome to PyTwoWay"},{"location":"fe-reference/","text":"fe_approximate_correction_full module Computes a bunch of estimates from an event study data set: - AKM variance decomposition - Andrews bias correction - KSS bias correction Does this through class FEsolver FEsolver Uses multigrid and partialing out to solve two way Fixed Effect model FIXME I think delete everything below this, it's basically contained in the class/functions within the class takes as an input this adata and creates associated A = [J W] matrix which are AKM dummies provides methods to do A x Y but also (A'A)^-1 A'Y solve method __getstate__ ( self ) special !!! purpose Defines how the model is pickled Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def __getstate__ ( self ): ''' Purpose: Defines how the model is pickled Arguments: Nothing Returns: Nothing ''' odict = { k : self . __dict__ [ k ] for k in self . __dict__ . keys () - { 'ml' }} return odict __init__ ( self , params ) special !!! purpose Initialize FEsolver object Parameters: Name Type Description Default params dictionary dictionary of parameters data (Pandas DataFrame): labor data. Contains the following columns: wid (worker id) y1 (compensation 1) y2 (compensation 2) f1i (firm id 1) f2i (firm id 2) m (0 if stayer, 1 if mover) ncore (int): number of cores to use ndraw_pii (int): number of draws to compute leverage ndraw_tr (int): number of draws to compute heteroskedastic correction hetero (bool): if True, compute heteroskedastic correction statsonly (bool): if True, return only basic statistics out (string): if statsonly is True, this is the file where the statistics will be saved batch (): #FIXME I don't know what this is required Returns: Type Description Object of type FEsolver Source code in pytwoway/fe_approximate_correction_full.py def __init__ ( self , params ): ''' Purpose: Initialize FEsolver object Arguments: params (dictionary): dictionary of parameters data (Pandas DataFrame): labor data. Contains the following columns: wid (worker id) y1 (compensation 1) y2 (compensation 2) f1i (firm id 1) f2i (firm id 2) m (0 if stayer, 1 if mover) ncore (int): number of cores to use ndraw_pii (int): number of draws to compute leverage ndraw_tr (int): number of draws to compute heteroskedastic correction hetero (bool): if True, compute heteroskedastic correction statsonly (bool): if True, return only basic statistics out (string): if statsonly is True, this is the file where the statistics will be saved batch (): #FIXME I don't know what this is Returns: Object of type FEsolver ''' start_time = time . time () self . params = params self . res = {} # Results dictionary # Save some commonly used parameters as attributes self . ncore = self . params [ 'ncore' ] # number of cores to use self . ndraw_pii = self . params [ 'ndraw_pii' ] # number of draws to compute leverage self . ndraw_trace = self . params [ 'ndraw_tr' ] # number of draws to compute hetero correction self . compute_hetero = self . params [ 'hetero' ] # Store some parameters in results dictionary self . res [ 'cores' ] = self . ncore self . res [ 'ndp' ] = self . ndraw_pii self . res [ 'ndt' ] = self . ndraw_trace # Begin cleaning and analysis self . prep_data () # Prepare data self . init_prepped_adata () # Use cleaned adata to generate some attributes self . compute_early_stats () # Use cleaned data to compute some statistics if self . params [ 'statsonly' ]: # If only returning early statistics self . save_early_stats () else : # If running analysis self . create_fe_solver () # Solve FE model self . compute_trace_approximation_fe () # Compute trace approxmation # If computing heteroskedastic correction if self . compute_hetero : self . compute_leverages_Pii () # Solve he model self . compute_trace_approximation_he () # Compute trace approximation self . collect_res () # Collect all results end_time = time . time () self . res [ 'total_time' ] = end_time - start_time self . save_res () # Save results to json logger . info ( '------ DONE -------' ) __setstate__ ( self , d ) special !!! purpose Defines how the model is unpickled Parameters: Name Type Description Default d dictionary attribute dictionary required Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def __setstate__ ( self , d ): ''' Purpose: Defines how the model is unpickled Arguments: d (dictionary): attribute dictionary Returns: Nothing ''' # Need to recreate the simple model and the search representation self . __dict__ = d # Make d the attribute dictionary self . ml = pyamg . ruge_stuben_solver ( self . M ) collect_res ( self ) !!! purpose Collect all results Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def collect_res ( self ): ''' Purpose: Collect all results Arguments: Nothing Returns: Nothing ''' self . res [ 'tot_var' ] = self . tot_var self . res [ 'eps_var_ho' ] = self . var_e self . res [ 'eps_var_fe' ] = np . var ( self . E ) self . res [ 'tr_var_ho' ] = np . mean ( self . tr_var_ho_all ) self . res [ 'tr_cov_ho' ] = np . mean ( self . tr_cov_ho_all ) logger . info ( '[ho] VAR tr= {:2.4f} (sd= {:2.4e} )' . format ( self . res [ 'tr_var_ho' ], np . std ( self . tr_var_ho_all ))) logger . info ( '[ho] COV tr= {:2.4f} (sd= {:2.4e} )' . format ( self . res [ 'tr_cov_ho' ], np . std ( self . tr_cov_ho_all ))) if self . compute_hetero : self . res [ 'eps_var_he' ] = self . Sii . mean () self . res [ 'min_lev' ] = self . adata . query ( 'm == 1' ) . Pii . min () self . res [ 'max_lev' ] = self . adata . query ( 'm == 1' ) . Pii . max () self . res [ 'tr_var_he' ] = np . mean ( self . tr_var_he_all ) self . res [ 'tr_cov_he' ] = np . mean ( self . tr_cov_he_all ) self . res [ 'tr_var_ho_sd' ] = np . std ( self . tr_var_ho_all ) self . res [ 'tr_cov_ho_sd' ] = np . std ( self . tr_cov_ho_all ) self . res [ 'tr_var_he_sd' ] = np . std ( self . tr_var_he_all ) self . res [ 'tr_cov_he_sd' ] = np . std ( self . tr_cov_he_all ) logger . info ( '[he] VAR tr= {:2.4f} (sd= {:2.4e} )' . format ( self . res [ 'tr_var_he' ], np . std ( self . tr_var_he_all ))) logger . info ( '[he] COV tr= {:2.4f} (sd= {:2.4e} )' . format ( self . res [ 'tr_cov_he' ], np . std ( self . tr_cov_he_all ))) # ----- FINAL ------ logger . info ( '[ho] VAR fe= {:2.4f} bc= {:2.4f} ' . format ( self . var_fe , self . var_fe - self . var_e * self . res [ 'tr_var_ho' ])) logger . info ( '[ho] COV fe= {:2.4f} bc= {:2.4f} ' . format ( self . cov_fe , self . cov_fe - self . var_e * self . res [ 'tr_cov_ho' ])) if self . compute_hetero : logger . info ( '[he] VAR fe= {:2.4f} bc= {:2.4f} ' . format ( self . var_fe , self . var_fe - self . res [ 'tr_var_he' ])) logger . info ( '[he] COV fe= {:2.4f} bc= {:2.4f} ' . format ( self . cov_fe , self . cov_fe - self . res [ 'tr_cov_he' ])) self . res [ 'var_y' ] = np . var ( self . Yq ) self . res [ 'var_fe' ] = self . var_fe self . res [ 'cov_fe' ] = self . cov_fe self . res [ 'var_ho' ] = self . var_fe - self . var_e * self . res [ 'tr_var_ho' ] self . res [ 'cov_ho' ] = self . cov_fe - self . var_e * self . res [ 'tr_cov_ho' ] if self . compute_hetero : self . res [ 'var_he' ] = self . var_fe - self . res [ 'tr_var_he' ] self . res [ 'cov_he' ] = self . cov_fe - self . res [ 'tr_cov_he' ] compute_early_stats ( self ) !!! purpose Compute some early statistics Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def compute_early_stats ( self ): ''' Purpose: Compute some early statistics Arguments: Nothing Returns: Nothing ''' fdata = self . adata . groupby ( 'f1i' ) . agg ({ 'm' : 'sum' , 'y1' : 'mean' , 'wid' : 'count' }) self . res [ 'mover_quantiles' ] = self . weighted_quantile ( fdata [ 'm' ], np . linspace ( 0 , 1 , 11 ), fdata [ 'wid' ]) . tolist () self . res [ 'size_quantiles' ] = self . weighted_quantile ( fdata [ 'wid' ], np . linspace ( 0 , 1 , 11 ), fdata [ 'wid' ]) . tolist () self . res [ 'between_firm_var' ] = self . weighted_var ( fdata [ 'y1' ], fdata [ 'wid' ]) self . res [ 'var_y' ] = self . adata [ self . adata [ 'cs' ] == 1 ][ 'y1' ] . var () # FIXME changed from adata.query('cs==1') (I ran %timeit and slicing is faster) compute_leverages_Pii ( self ) !!! purpose Compute leverages for heteroskedastic correction Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def compute_leverages_Pii ( self ): ''' Purpose: Compute leverages for heteroskedastic correction Arguments: Nothing Returns: Nothing ''' self . Pii = np . zeros ( self . nn ) self . Sii = np . zeros ( self . nn ) if len ( self . params [ 'levfile' ]) > 1 : logger . info ( '[he] starting heteroskedastic correction, loading precomputed files' ) files = glob . glob ( ' {} *' . format ( self . params [ 'levfile' ])) logger . info ( '[he] found {} files to get leverages from' . format ( len ( files ))) self . res [ 'lev_file_count' ] = len ( files ) assert len ( files ) > 0 , \"Didn't find any leverage files!\" for f in files : pp = np . load ( f ) self . Pii += pp / len ( files ) elif self . ncore > 1 : logger . info ( '[he] starting heteroskedastic correction p2= {} , using {} cores, batch size {} ' . format ( self . ndraw_pii , self . ncore , self . params [ 'batch' ])) set_start_method ( 'spawn' ) with Pool ( processes = self . ncore ) as pool : Pii_all = pool . starmap ( self . leverage_approx , [ self . params [ 'batch' ] for _ in range ( self . ndraw_pii // self . params [ 'batch' ])]) for pp in Pii_all : Pii += pp / len ( Pii_all ) else : Pii_all = list ( itertools . starmap ( self . leverage_approx , [ self . params [ 'batch' ] for _ in range ( self . ndraw_pii // self . params [ 'batch' ])])) for pp in Pii_all : self . Pii += pp / len ( Pii_all ) I = 1.0 * self . adata . eval ( 'm == 1' ) max_leverage = ( I * self . Pii ) . max () # Attach the computed Pii to the dataframe self . adata [ 'Pii' ] = self . Pii logger . info ( '[he] Leverage range {:2.4f} to {:2.4f} ' . format ( self . adata . query ( 'm == 1' ) . Pii . min (), self . adata . query ( 'm == 1' ) . Pii . max ())) # Give stayers the variance estimate at the firm level self . adata [ 'Sii' ] = self . Y * self . E / ( 1 - self . Pii ) S_j = self . adata . query ( 'm == 1' ) . rename ( columns = { 'Sii' : 'Sii_j' }) . groupby ( 'f1i' )[ 'Sii_j' ] . agg ( 'mean' ) self . adata = pd . merge ( self . adata , S_j , on = 'f1i' ) self . adata [ 'Sii' ] = np . where ( self . adata [ 'm' ] == 1 , self . adata [ 'Sii' ], self . adata [ 'Sii_j' ]) self . Sii = self . adata [ 'Sii' ] logger . info ( '[he] variance of residuals in heteroskedastic case: {:2.4f} ' . format ( self . Sii . mean ())) compute_trace_approximation_fe ( self ) !!! purpose Compute FE trace approximation Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def compute_trace_approximation_fe ( self ): ''' Purpose: Compute FE trace approximation Arguments: Nothing Returns: Nothing ''' logger . info ( 'Starting FE trace correction ndraws= {} , using {} cores' . format ( self . ndraw_trace , self . ncore )) self . tr_var_ho_all = np . zeros ( self . ndraw_trace ) self . tr_cov_ho_all = np . zeros ( self . ndraw_trace ) for r in trange ( self . ndraw_trace ): # Generate -1 or 1 Zpsi = 2 * np . random . binomial ( 1 , 0.5 , self . nf - 1 ) - 1 Zalpha = 2 * np . random . binomial ( 1 , 0.5 , self . nw ) - 1 R1 = self . Jq * Zpsi psi1 , alpha1 = self . mult_AAinv ( Zpsi , Zalpha ) R2_psi = self . Jq * psi1 R2_alpha = self . Wq * alpha1 # Trace corrections self . tr_var_ho_all [ r ] = np . cov ( R1 , R2_psi )[ 0 ][ 1 ] self . tr_cov_ho_all [ r ] = np . cov ( R1 , R2_alpha )[ 0 ][ 1 ] logger . debug ( 'FE [traces] step {} / {} done.' . format ( r , self . ndraw_trace )) compute_trace_approximation_he ( self ) !!! purpose Compute heteroskedastic trace approximation Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def compute_trace_approximation_he ( self ): ''' Purpose: Compute heteroskedastic trace approximation Arguments: Nothing Returns: Nothing ''' logger . info ( 'Starting he trace correction ndraws= {} , using {} cores' . format ( self . ndraw_trace , self . ncore )) self . tr_var_he_all = np . zeros ( self . ndraw_trace ) self . tr_cov_he_all = np . zeros ( self . ndraw_trace ) for r in trange ( self . ndraw_trace ): Zpsi = 2 * np . random . binomial ( 1 , 0.5 , self . nf - 1 ) - 1 Zalpha = 2 * np . random . binomial ( 1 , 0.5 , self . nw ) - 1 psi1 , alpha1 = self . mult_AAinv ( Zpsi , Zalpha ) R2_psi = self . Jq * psi1 R2_alpha = self . Wq * alpha1 psi2 , alpha2 = self . mult_AAinv ( * self . mult_Atranspose ( self . Sii * self . mult_A ( Zpsi , Zalpha ))) R3_psi = self . Jq * psi2 # Trace corrections self . tr_var_he_all [ r ] = np . cov ( R2_psi , R3_psi )[ 0 ][ 1 ] self . tr_cov_he_all [ r ] = np . cov ( R2_alpha , R3_psi )[ 0 ][ 1 ] logger . debug ( 'he [traces] step {} / {} done.' . format ( r , self . ndraw_trace )) create_fe_solver ( self ) !!! purpose Solve FE model Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def create_fe_solver ( self ): ''' Purpose: Solve FE model Arguments: Nothing Returns: Nothing ''' self . Y = self . adata . y1 # try to pickle the object to see its size # self.save('tmp.pkl') # FIXME should we delete these 2 lines? logger . info ( 'Extract firm effects' ) psi_hat , alpha_hat = self . solve ( self . Y ) logger . info ( 'Solver time {:2.4f} seconds' . format ( self . last_invert_time )) logger . info ( 'Expected total time {:2.4f} minutes' . format ( ( self . ndraw_trace * ( 1 + self . compute_hetero ) + self . ndraw_pii * self . compute_hetero ) * self . last_invert_time / 60 )) self . E = self . Y - self . mult_A ( psi_hat , alpha_hat ) self . res [ 'solver_time' ] = self . last_invert_time fe_rsq = 1 - np . power ( self . E , 2 ) . mean () / np . power ( self . Y , 2 ) . mean () logger . info ( 'Fixed effect R-square {:2.4f} ' . format ( fe_rsq )) self . var_fe = np . var ( self . Jq * psi_hat ) self . cov_fe = np . cov ( self . Jq * psi_hat , self . Wq * alpha_hat )[ 0 ][ 1 ] self . tot_var = np . var ( self . Y ) logger . info ( '[fe] var_psi= {:2.4f} cov= {:2.4f} tot= {:2.4f} ' . format ( self . var_fe , self . cov_fe , self . tot_var )) self . var_e = self . nn / ( self . nn - self . nw - self . nf + 1 ) * np . power ( self . E , 2 ) . mean () logger . info ( '[ho] variance of residuals {:2.4f} ' . format ( self . var_e )) init_prepped_adata ( self ) !!! purpose Use prepped adata to initialize class attributes Parameters: Name Type Description Default adata Pandas DataFrame labor data. Contains the following columns: wid (worker id) y1 (compensation 1) y2 (compensation 2) f1i (firm id 1) f2i (firm id 2) m (0 if stayer, 1 if mover) required Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def init_prepped_adata ( self ): ''' Purpose: Use prepped adata to initialize class attributes Arguments: adata (Pandas DataFrame): labor data. Contains the following columns: wid (worker id) y1 (compensation 1) y2 (compensation 2) f1i (firm id 1) f2i (firm id 2) m (0 if stayer, 1 if mover) Returns: Nothing ''' nf = self . adata . f1i . max () # Number of firms nw = self . adata . wid . max () # Number of workers nn = len ( self . adata ) # Number of observations self . nf = nf self . nw = nw self . nn = nn logger . info ( 'data nf: {} nw: {} nn: {} ' . format ( nf , nw , nn )) # Matrices for the cross-section J = csc_matrix (( np . ones ( nn ), ( self . adata . index , self . adata . f1i - 1 )), shape = ( nn , nf )) # Firms J = J [:, range ( nf - 1 )] # Normalize one firm to 0 self . J = J W = csc_matrix (( np . ones ( nn ), ( self . adata . index , self . adata . wid - 1 )), shape = ( nn , nw )) # Workers self . W = W # Dw = diags((W.T * W).diagonal()) # FIXME changed from .transpose() to .T ALSO commented this out since it's not used Dwinv = diags ( 1.0 / (( W . T * W ) . diagonal ())) # FIXME changed from .transpose() to .T self . Dwinv = Dwinv logger . info ( 'Prepare linear solver' ) # Finally create M M = J . T * J - J . T * W * Dwinv * W . T * J # FIXME changed from .transpose() to .T self . M = M self . ml = pyamg . ruge_stuben_solver ( M ) # L = diags(fes.M.diagonal()) - fes.M # r = linalg.eigsh(L,k=2,which='LM') # Create cross-section matrices # cs == 1 ==> looking at y1 for movers (cs = cross section) mdata = self . adata [ self . adata [ 'cs' ] == 1 ] # FIXME changed from adata.query('cs==1') (I ran %timeit and slicing is faster) mdata = mdata . reset_index ( drop = True ) # FIXME changed from set_index(pd.Series(range(len(mdata)))) nnq = len ( mdata ) # Number of observations self . nnq = nnq Jq = csc_matrix (( np . ones ( nnq ), ( mdata . index , mdata . f1i - 1 )), shape = ( nnq , nf )) self . Jq = Jq [:, range ( nf - 1 )] # normalizing one firm to 0 self . Wq = csc_matrix (( np . ones ( nnq ), ( mdata . index , mdata . wid - 1 )), shape = ( nnq , nw )) self . Yq = mdata [ 'y1' ] # Save time variable self . last_invert_time = 0 leverage_approx ( self , ndraw_pii ) !!! purpose Compute an approximate leverage using ndraw_pii Parameters: Name Type Description Default ndraw_pii int number of draws required Returns: Type Description Pii (NumPy Array) FIXME I don't know what this function does, so I don't know what Pii is Source code in pytwoway/fe_approximate_correction_full.py def leverage_approx ( self , ndraw_pii ): ''' Purpose: Compute an approximate leverage using ndraw_pii Arguments: ndraw_pii (int): number of draws Returns: Pii (NumPy Array): # FIXME I don't know what this function does, so I don't know what Pii is ''' Pii = np . zeros ( self . nn ) # Compute the different draws for r in trange ( ndraw_pii ): R2 = 2 * np . random . binomial ( 1 , 0.5 , self . nn ) - 1 Pii += 1 / ndraw_pii * np . power ( self . proj ( R2 ), 2.0 ) logger . info ( 'Done with batch' ) return Pii load ( filename ) staticmethod !!! purpose Load files for heteroskedastic correction Parameters: Name Type Description Default filename string file to load required Returns: Type Description fes loaded file Source code in pytwoway/fe_approximate_correction_full.py @staticmethod def load ( filename ): ''' Purpose: Load files for heteroskedastic correction Arguments: filename (string): file to load Returns: fes: loaded file ''' fes = None with open ( filename , 'rb' ) as infile : fes = pickle . load ( infile ) return fes mult_A ( self , psi , alpha ) !!! purpose Multiplies A = [J W] stored in the object by psi and alpha (used, for example, to compute estimated outcomes and sample errors) Parameters: Name Type Description Default psi firm fixed effects # FIXME correct datatype required alpha worker fixed effects # FIXME correct datatype required Returns: Type Description J_psi + W_alpha (CSC Matrix) firms * firm fixed effects + workers * worker fixed effects Source code in pytwoway/fe_approximate_correction_full.py def mult_A ( self , psi , alpha ): ''' Purpose: Multiplies A = [J W] stored in the object by psi and alpha (used, for example, to compute estimated outcomes and sample errors) Arguments: psi: firm fixed effects # FIXME correct datatype alpha: worker fixed effects # FIXME correct datatype Returns: J_psi + W_alpha (CSC Matrix): firms * firm fixed effects + workers * worker fixed effects ''' # J_psi = self.J * psi # W_alpha = self.W * alpha return self . J * psi + self . W * alpha # J_psi + W_alpha mult_AAinv ( self , psi , alpha ) !!! purpose Multiplies gamma = [psi;alpha] by (A'A)^(-1) where A = [J W] stored in the object Parameters: Name Type Description Default psi firm fixed effects # FIXME correct datatype required alpha worker fixed effects # FIXME correct datatype required Returns: Type Description psi_out estimated firm fixed effects # FIXME correct datatype alpha_out : estimated worker fixed effects # FIXME correct datatype Source code in pytwoway/fe_approximate_correction_full.py def mult_AAinv ( self , psi , alpha ): ''' Purpose: Multiplies gamma = [psi;alpha] by (A'A)^(-1) where A = [J W] stored in the object Arguments: psi: firm fixed effects # FIXME correct datatype alpha: worker fixed effects # FIXME correct datatype Returns: psi_out: estimated firm fixed effects # FIXME correct datatype alpha_out : estimated worker fixed effects # FIXME correct datatype ''' # inter1 = self.ml.solve( psi , tol=1e-10 ) # inter2 = self.ml.solve( , tol=1e-10 ) # psi_out = inter1 - inter2 start = timer () psi_out = self . ml . solve ( psi - self . J . T * ( self . W * ( self . Dwinv * alpha )), tol = 1e-10 ) # FIXME changed from .transpose() to .T self . last_invert_time = timer () - start alpha_out = - self . Dwinv * ( self . W . T * ( self . J * psi_out )) + self . Dwinv * alpha # FIXME changed from .transpose() to .T return psi_out , alpha_out mult_Atranspose ( self , v ) !!! purpose Multiplies the transpose of A = [J W] stored in the object by v Parameters: Name Type Description Default v what to multiply by # FIXME correct datatype required Returns: Type Description J_transpose_V (CSC Matrix) firms * v W_transpose_V (CSC Matrix): workers * v Source code in pytwoway/fe_approximate_correction_full.py def mult_Atranspose ( self , v ): ''' Purpose: Multiplies the transpose of A = [J W] stored in the object by v Arguments: v: what to multiply by # FIXME correct datatype Returns: J_transpose_V (CSC Matrix): firms * v W_transpose_V (CSC Matrix): workers * v ''' # J_transpose_V = self.J.T * v # FIXME changed from .transpose() to .T # W_transpose_V = self.W.T * v # FIXME changed from .transpose() to .T return self . J . T * v , self . W . T * v # J_transpose_V, W_transpose_V prep_data ( self ) !!! purpose Do some initial data cleaning Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def prep_data ( self ): ''' Purpose: Do some initial data cleaning Arguments: Nothing Returns: Nothing ''' logger . info ( 'Preparing the data' ) data = self . params [ 'data' ] sdata = data [ data [ 'm' ] == 0 ] . reset_index ( drop = True ) jdata = data [ data [ 'm' ] == 1 ] . reset_index ( drop = True ) logger . info ( 'Data movers= {} stayers= {} ' . format ( len ( jdata ), len ( sdata ))) self . res [ 'nm' ] = len ( jdata ) self . res [ 'ns' ] = len ( sdata ) self . res [ 'n_firms' ] = len ( np . unique ( pd . concat ([ jdata [ 'f1i' ], jdata [ 'f2i' ], sdata [ 'f1i' ]], ignore_index = True ))) self . res [ 'n_workers' ] = len ( np . unique ( pd . concat ([ jdata [ 'wid' ], sdata [ 'wid' ]], ignore_index = True ))) self . res [ 'n_movers' ] = len ( np . unique ( pd . concat ([ jdata [ 'wid' ]], ignore_index = True ))) #res['year_max'] = int(sdata['year'].max()) #res['year_min'] = int(sdata['year'].min()) # Make wids unique per row jdata . set_index ( np . arange ( self . res [ 'nm' ]) + 1 ) sdata . set_index ( np . arange ( self . res [ 'ns' ]) + 1 + self . res [ 'nm' ]) jdata [ 'wid' ] = np . arange ( self . res [ 'nm' ]) + 1 sdata [ 'wid' ] = np . arange ( self . res [ 'ns' ]) + 1 + self . res [ 'nm' ] # Combine the 2 data-sets self . adata = pd . concat ([ sdata [[ 'wid' , 'f1i' , 'y1' ]] . assign ( cs = 1 , m = 0 ), jdata [[ 'wid' , 'f1i' , 'y1' ]] . assign ( cs = 1 , m = 1 ), jdata [[ 'wid' , 'f2i' , 'y2' ]] . rename ( columns = { 'f2i' : 'f1i' , 'y2' : 'y1' }) . assign ( cs = 0 , m = 1 )]) self . adata = self . adata . reset_index ( drop = True ) # FIXME changed from set_index(pd.Series(range(len(self.adata)))) self . adata [ 'wid' ] = self . adata [ 'wid' ] . astype ( 'category' ) . cat . codes + 1 proj ( self , y ) !!! purpose Solve y, then project onto X space of data stored in the object Parameters: Name Type Description Default y Pandas DataFrame labor data required Returns: Type Description Projection of psi, alpha solved from y onto X space Source code in pytwoway/fe_approximate_correction_full.py def proj ( self , y ): # FIXME should this y be Y? ''' Purpose: Solve y, then project onto X space of data stored in the object Arguments: y (Pandas DataFrame): labor data Returns: Projection of psi, alpha solved from y onto X space ''' return self . mult_A ( * self . solve ( y )) save ( self , filename ) !!! purpose Save FEsolver class to filename as pickle Parameters: Name Type Description Default filename string filename to save to required Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def save ( self , filename ): ''' Purpose: Save FEsolver class to filename as pickle Arguments: filename (string): filename to save to Returns: Nothing ''' with open ( filename , 'wb' ) as outfile : pickle . dump ( self , outfile ) save_early_stats ( self ) !!! purpose Save the early statistics computed in compute_early_stats() Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def save_early_stats ( self ): ''' Purpose: Save the early statistics computed in compute_early_stats() Arguments: Nothing Returns: Nothing ''' with open ( self . params [ 'out' ], 'w' ) as outfile : json . dump ( self . res , outfile ) logger . info ( 'saved results to {} ' . format ( self . params [ 'out' ])) logger . info ( '--statsonly was passed as argument, so we skip all estimation.' ) logger . info ( '------ DONE -------' ) save_res ( self ) !!! purpose Save results as json Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def save_res ( self ): ''' Purpose: Save results as json Arguments: Nothing Returns: Nothing ''' with open ( self . params [ 'out' ], 'w' ) as outfile : json . dump ( self . res , outfile ) logger . info ( 'Saved results to {} ' . format ( self . params [ 'out' ])) solve ( self , Y ) !!! purpose Compute (A'A)^-1 A'Y, the least squares estimate of A [psi_hat, alpha_hat] = Y Parameters: Name Type Description Default Y Pandas DataFrame labor data required Returns: Type Description psi_hat estimated firm fixed effects # FIXME correct datatype alpha_hat: estimated worker fixed effects # FIXME correct datatype Source code in pytwoway/fe_approximate_correction_full.py def solve ( self , Y ): ''' Purpose: Compute (A'A)^-1 A'Y, the least squares estimate of A [psi_hat, alpha_hat] = Y Arguments: Y (Pandas DataFrame): labor data Returns: psi_hat: estimated firm fixed effects # FIXME correct datatype alpha_hat: estimated worker fixed effects # FIXME correct datatype ''' J_transpose_Y , W_transpose_Y = self . mult_Atranspose ( Y ) # This gives A'Y psi_hat , alpha_hat = self . mult_AAinv ( J_transpose_Y , W_transpose_Y ) return psi_hat , alpha_hat weighted_quantile ( self , values , quantiles , sample_weight = None , values_sorted = False , old_style = False ) !!! purpose Very close to numpy.percentile, but supports weights. NOTE: quantiles should be in [0, 1]! Parameters: Name Type Description Default param values: numpy.array with data required param quantiles: array-like with many quantiles needed required param sample_weight: array-like of the same length as array required param values_sorted: bool, if True, then will avoid sorting of initial array required param old_style: if True, will correct output to be consistent with numpy.percentile. required Returns: Type Description return: numpy.array with computed quantiles. Source code in pytwoway/fe_approximate_correction_full.py def weighted_quantile ( self , values , quantiles , sample_weight = None , values_sorted = False , old_style = False ): # FIXME was formerly a function outside the class ''' Purpose: Very close to numpy.percentile, but supports weights. NOTE: quantiles should be in [0, 1]! Arguments: :param values: numpy.array with data :param quantiles: array-like with many quantiles needed :param sample_weight: array-like of the same length as `array` :param values_sorted: bool, if True, then will avoid sorting of initial array :param old_style: if True, will correct output to be consistent with numpy.percentile. Returns: :return: numpy.array with computed quantiles. ''' values = np . array ( values ) quantiles = np . array ( quantiles ) if sample_weight is None : sample_weight = np . ones ( len ( values )) sample_weight = np . array ( sample_weight ) assert np . all ( quantiles >= 0 ) and np . all ( quantiles <= 1 ), \\ 'quantiles should be in [0, 1]' if not values_sorted : sorter = np . argsort ( values ) values = values [ sorter ] sample_weight = sample_weight [ sorter ] weighted_quantiles = np . cumsum ( sample_weight ) - 0.5 * sample_weight if old_style : # To be convenient with numpy.percentile weighted_quantiles -= weighted_quantiles [ 0 ] weighted_quantiles /= weighted_quantiles [ - 1 ] else : weighted_quantiles /= np . sum ( sample_weight ) return np . interp ( quantiles , weighted_quantiles , values ) weighted_var ( self , v , w ) !!! purpose Compute weighted variance # FIXME I don't know what this function really does Parameters: Name Type Description Default v FIXME I don't know what this is required w FIXME I don't know what this is required Returns: Type Description v0 FIXME I don't know what this is Source code in pytwoway/fe_approximate_correction_full.py def weighted_var ( self , v , w ): # FIXME was formerly a function outside the class ''' Purpose: Compute weighted variance # FIXME I don't know what this function really does Arguments: v: # FIXME I don't know what this is w: # FIXME I don't know what this is Returns: v0: # FIXME I don't know what this is ''' m0 = np . sum ( w * v ) / np . sum ( w ) v0 = np . sum ( w * ( v - m0 ) ** 2 ) / np . sum ( w ) return v0","title":"fe_approximate_correction_full"},{"location":"fe-reference/#fe_approximate_correction_full-module","text":"","title":"fe_approximate_correction_full module"},{"location":"fe-reference/#fe_approximate_correction_full","text":"Computes a bunch of estimates from an event study data set: - AKM variance decomposition - Andrews bias correction - KSS bias correction Does this through class FEsolver","title":"fe_approximate_correction_full"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver","text":"Uses multigrid and partialing out to solve two way Fixed Effect model","title":"FEsolver"},{"location":"fe-reference/#fixme-i-think-delete-everything-below-this-its-basically-contained-in-the-classfunctions-within-the-class","text":"takes as an input this adata and creates associated A = [J W] matrix which are AKM dummies provides methods to do A x Y but also (A'A)^-1 A'Y solve method","title":"FIXME I think delete everything below this, it's basically contained in the class/functions within the class"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.__getstate__","text":"!!! purpose Defines how the model is pickled Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def __getstate__ ( self ): ''' Purpose: Defines how the model is pickled Arguments: Nothing Returns: Nothing ''' odict = { k : self . __dict__ [ k ] for k in self . __dict__ . keys () - { 'ml' }} return odict","title":"__getstate__()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.__init__","text":"!!! purpose Initialize FEsolver object Parameters: Name Type Description Default params dictionary dictionary of parameters data (Pandas DataFrame): labor data. Contains the following columns: wid (worker id) y1 (compensation 1) y2 (compensation 2) f1i (firm id 1) f2i (firm id 2) m (0 if stayer, 1 if mover) ncore (int): number of cores to use ndraw_pii (int): number of draws to compute leverage ndraw_tr (int): number of draws to compute heteroskedastic correction hetero (bool): if True, compute heteroskedastic correction statsonly (bool): if True, return only basic statistics out (string): if statsonly is True, this is the file where the statistics will be saved batch (): #FIXME I don't know what this is required Returns: Type Description Object of type FEsolver Source code in pytwoway/fe_approximate_correction_full.py def __init__ ( self , params ): ''' Purpose: Initialize FEsolver object Arguments: params (dictionary): dictionary of parameters data (Pandas DataFrame): labor data. Contains the following columns: wid (worker id) y1 (compensation 1) y2 (compensation 2) f1i (firm id 1) f2i (firm id 2) m (0 if stayer, 1 if mover) ncore (int): number of cores to use ndraw_pii (int): number of draws to compute leverage ndraw_tr (int): number of draws to compute heteroskedastic correction hetero (bool): if True, compute heteroskedastic correction statsonly (bool): if True, return only basic statistics out (string): if statsonly is True, this is the file where the statistics will be saved batch (): #FIXME I don't know what this is Returns: Object of type FEsolver ''' start_time = time . time () self . params = params self . res = {} # Results dictionary # Save some commonly used parameters as attributes self . ncore = self . params [ 'ncore' ] # number of cores to use self . ndraw_pii = self . params [ 'ndraw_pii' ] # number of draws to compute leverage self . ndraw_trace = self . params [ 'ndraw_tr' ] # number of draws to compute hetero correction self . compute_hetero = self . params [ 'hetero' ] # Store some parameters in results dictionary self . res [ 'cores' ] = self . ncore self . res [ 'ndp' ] = self . ndraw_pii self . res [ 'ndt' ] = self . ndraw_trace # Begin cleaning and analysis self . prep_data () # Prepare data self . init_prepped_adata () # Use cleaned adata to generate some attributes self . compute_early_stats () # Use cleaned data to compute some statistics if self . params [ 'statsonly' ]: # If only returning early statistics self . save_early_stats () else : # If running analysis self . create_fe_solver () # Solve FE model self . compute_trace_approximation_fe () # Compute trace approxmation # If computing heteroskedastic correction if self . compute_hetero : self . compute_leverages_Pii () # Solve he model self . compute_trace_approximation_he () # Compute trace approximation self . collect_res () # Collect all results end_time = time . time () self . res [ 'total_time' ] = end_time - start_time self . save_res () # Save results to json logger . info ( '------ DONE -------' )","title":"__init__()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.__setstate__","text":"!!! purpose Defines how the model is unpickled Parameters: Name Type Description Default d dictionary attribute dictionary required Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def __setstate__ ( self , d ): ''' Purpose: Defines how the model is unpickled Arguments: d (dictionary): attribute dictionary Returns: Nothing ''' # Need to recreate the simple model and the search representation self . __dict__ = d # Make d the attribute dictionary self . ml = pyamg . ruge_stuben_solver ( self . M )","title":"__setstate__()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.collect_res","text":"!!! purpose Collect all results Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def collect_res ( self ): ''' Purpose: Collect all results Arguments: Nothing Returns: Nothing ''' self . res [ 'tot_var' ] = self . tot_var self . res [ 'eps_var_ho' ] = self . var_e self . res [ 'eps_var_fe' ] = np . var ( self . E ) self . res [ 'tr_var_ho' ] = np . mean ( self . tr_var_ho_all ) self . res [ 'tr_cov_ho' ] = np . mean ( self . tr_cov_ho_all ) logger . info ( '[ho] VAR tr= {:2.4f} (sd= {:2.4e} )' . format ( self . res [ 'tr_var_ho' ], np . std ( self . tr_var_ho_all ))) logger . info ( '[ho] COV tr= {:2.4f} (sd= {:2.4e} )' . format ( self . res [ 'tr_cov_ho' ], np . std ( self . tr_cov_ho_all ))) if self . compute_hetero : self . res [ 'eps_var_he' ] = self . Sii . mean () self . res [ 'min_lev' ] = self . adata . query ( 'm == 1' ) . Pii . min () self . res [ 'max_lev' ] = self . adata . query ( 'm == 1' ) . Pii . max () self . res [ 'tr_var_he' ] = np . mean ( self . tr_var_he_all ) self . res [ 'tr_cov_he' ] = np . mean ( self . tr_cov_he_all ) self . res [ 'tr_var_ho_sd' ] = np . std ( self . tr_var_ho_all ) self . res [ 'tr_cov_ho_sd' ] = np . std ( self . tr_cov_ho_all ) self . res [ 'tr_var_he_sd' ] = np . std ( self . tr_var_he_all ) self . res [ 'tr_cov_he_sd' ] = np . std ( self . tr_cov_he_all ) logger . info ( '[he] VAR tr= {:2.4f} (sd= {:2.4e} )' . format ( self . res [ 'tr_var_he' ], np . std ( self . tr_var_he_all ))) logger . info ( '[he] COV tr= {:2.4f} (sd= {:2.4e} )' . format ( self . res [ 'tr_cov_he' ], np . std ( self . tr_cov_he_all ))) # ----- FINAL ------ logger . info ( '[ho] VAR fe= {:2.4f} bc= {:2.4f} ' . format ( self . var_fe , self . var_fe - self . var_e * self . res [ 'tr_var_ho' ])) logger . info ( '[ho] COV fe= {:2.4f} bc= {:2.4f} ' . format ( self . cov_fe , self . cov_fe - self . var_e * self . res [ 'tr_cov_ho' ])) if self . compute_hetero : logger . info ( '[he] VAR fe= {:2.4f} bc= {:2.4f} ' . format ( self . var_fe , self . var_fe - self . res [ 'tr_var_he' ])) logger . info ( '[he] COV fe= {:2.4f} bc= {:2.4f} ' . format ( self . cov_fe , self . cov_fe - self . res [ 'tr_cov_he' ])) self . res [ 'var_y' ] = np . var ( self . Yq ) self . res [ 'var_fe' ] = self . var_fe self . res [ 'cov_fe' ] = self . cov_fe self . res [ 'var_ho' ] = self . var_fe - self . var_e * self . res [ 'tr_var_ho' ] self . res [ 'cov_ho' ] = self . cov_fe - self . var_e * self . res [ 'tr_cov_ho' ] if self . compute_hetero : self . res [ 'var_he' ] = self . var_fe - self . res [ 'tr_var_he' ] self . res [ 'cov_he' ] = self . cov_fe - self . res [ 'tr_cov_he' ]","title":"collect_res()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.compute_early_stats","text":"!!! purpose Compute some early statistics Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def compute_early_stats ( self ): ''' Purpose: Compute some early statistics Arguments: Nothing Returns: Nothing ''' fdata = self . adata . groupby ( 'f1i' ) . agg ({ 'm' : 'sum' , 'y1' : 'mean' , 'wid' : 'count' }) self . res [ 'mover_quantiles' ] = self . weighted_quantile ( fdata [ 'm' ], np . linspace ( 0 , 1 , 11 ), fdata [ 'wid' ]) . tolist () self . res [ 'size_quantiles' ] = self . weighted_quantile ( fdata [ 'wid' ], np . linspace ( 0 , 1 , 11 ), fdata [ 'wid' ]) . tolist () self . res [ 'between_firm_var' ] = self . weighted_var ( fdata [ 'y1' ], fdata [ 'wid' ]) self . res [ 'var_y' ] = self . adata [ self . adata [ 'cs' ] == 1 ][ 'y1' ] . var () # FIXME changed from adata.query('cs==1') (I ran %timeit and slicing is faster)","title":"compute_early_stats()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.compute_leverages_Pii","text":"!!! purpose Compute leverages for heteroskedastic correction Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def compute_leverages_Pii ( self ): ''' Purpose: Compute leverages for heteroskedastic correction Arguments: Nothing Returns: Nothing ''' self . Pii = np . zeros ( self . nn ) self . Sii = np . zeros ( self . nn ) if len ( self . params [ 'levfile' ]) > 1 : logger . info ( '[he] starting heteroskedastic correction, loading precomputed files' ) files = glob . glob ( ' {} *' . format ( self . params [ 'levfile' ])) logger . info ( '[he] found {} files to get leverages from' . format ( len ( files ))) self . res [ 'lev_file_count' ] = len ( files ) assert len ( files ) > 0 , \"Didn't find any leverage files!\" for f in files : pp = np . load ( f ) self . Pii += pp / len ( files ) elif self . ncore > 1 : logger . info ( '[he] starting heteroskedastic correction p2= {} , using {} cores, batch size {} ' . format ( self . ndraw_pii , self . ncore , self . params [ 'batch' ])) set_start_method ( 'spawn' ) with Pool ( processes = self . ncore ) as pool : Pii_all = pool . starmap ( self . leverage_approx , [ self . params [ 'batch' ] for _ in range ( self . ndraw_pii // self . params [ 'batch' ])]) for pp in Pii_all : Pii += pp / len ( Pii_all ) else : Pii_all = list ( itertools . starmap ( self . leverage_approx , [ self . params [ 'batch' ] for _ in range ( self . ndraw_pii // self . params [ 'batch' ])])) for pp in Pii_all : self . Pii += pp / len ( Pii_all ) I = 1.0 * self . adata . eval ( 'm == 1' ) max_leverage = ( I * self . Pii ) . max () # Attach the computed Pii to the dataframe self . adata [ 'Pii' ] = self . Pii logger . info ( '[he] Leverage range {:2.4f} to {:2.4f} ' . format ( self . adata . query ( 'm == 1' ) . Pii . min (), self . adata . query ( 'm == 1' ) . Pii . max ())) # Give stayers the variance estimate at the firm level self . adata [ 'Sii' ] = self . Y * self . E / ( 1 - self . Pii ) S_j = self . adata . query ( 'm == 1' ) . rename ( columns = { 'Sii' : 'Sii_j' }) . groupby ( 'f1i' )[ 'Sii_j' ] . agg ( 'mean' ) self . adata = pd . merge ( self . adata , S_j , on = 'f1i' ) self . adata [ 'Sii' ] = np . where ( self . adata [ 'm' ] == 1 , self . adata [ 'Sii' ], self . adata [ 'Sii_j' ]) self . Sii = self . adata [ 'Sii' ] logger . info ( '[he] variance of residuals in heteroskedastic case: {:2.4f} ' . format ( self . Sii . mean ()))","title":"compute_leverages_Pii()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.compute_trace_approximation_fe","text":"!!! purpose Compute FE trace approximation Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def compute_trace_approximation_fe ( self ): ''' Purpose: Compute FE trace approximation Arguments: Nothing Returns: Nothing ''' logger . info ( 'Starting FE trace correction ndraws= {} , using {} cores' . format ( self . ndraw_trace , self . ncore )) self . tr_var_ho_all = np . zeros ( self . ndraw_trace ) self . tr_cov_ho_all = np . zeros ( self . ndraw_trace ) for r in trange ( self . ndraw_trace ): # Generate -1 or 1 Zpsi = 2 * np . random . binomial ( 1 , 0.5 , self . nf - 1 ) - 1 Zalpha = 2 * np . random . binomial ( 1 , 0.5 , self . nw ) - 1 R1 = self . Jq * Zpsi psi1 , alpha1 = self . mult_AAinv ( Zpsi , Zalpha ) R2_psi = self . Jq * psi1 R2_alpha = self . Wq * alpha1 # Trace corrections self . tr_var_ho_all [ r ] = np . cov ( R1 , R2_psi )[ 0 ][ 1 ] self . tr_cov_ho_all [ r ] = np . cov ( R1 , R2_alpha )[ 0 ][ 1 ] logger . debug ( 'FE [traces] step {} / {} done.' . format ( r , self . ndraw_trace ))","title":"compute_trace_approximation_fe()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.compute_trace_approximation_he","text":"!!! purpose Compute heteroskedastic trace approximation Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def compute_trace_approximation_he ( self ): ''' Purpose: Compute heteroskedastic trace approximation Arguments: Nothing Returns: Nothing ''' logger . info ( 'Starting he trace correction ndraws= {} , using {} cores' . format ( self . ndraw_trace , self . ncore )) self . tr_var_he_all = np . zeros ( self . ndraw_trace ) self . tr_cov_he_all = np . zeros ( self . ndraw_trace ) for r in trange ( self . ndraw_trace ): Zpsi = 2 * np . random . binomial ( 1 , 0.5 , self . nf - 1 ) - 1 Zalpha = 2 * np . random . binomial ( 1 , 0.5 , self . nw ) - 1 psi1 , alpha1 = self . mult_AAinv ( Zpsi , Zalpha ) R2_psi = self . Jq * psi1 R2_alpha = self . Wq * alpha1 psi2 , alpha2 = self . mult_AAinv ( * self . mult_Atranspose ( self . Sii * self . mult_A ( Zpsi , Zalpha ))) R3_psi = self . Jq * psi2 # Trace corrections self . tr_var_he_all [ r ] = np . cov ( R2_psi , R3_psi )[ 0 ][ 1 ] self . tr_cov_he_all [ r ] = np . cov ( R2_alpha , R3_psi )[ 0 ][ 1 ] logger . debug ( 'he [traces] step {} / {} done.' . format ( r , self . ndraw_trace ))","title":"compute_trace_approximation_he()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.create_fe_solver","text":"!!! purpose Solve FE model Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def create_fe_solver ( self ): ''' Purpose: Solve FE model Arguments: Nothing Returns: Nothing ''' self . Y = self . adata . y1 # try to pickle the object to see its size # self.save('tmp.pkl') # FIXME should we delete these 2 lines? logger . info ( 'Extract firm effects' ) psi_hat , alpha_hat = self . solve ( self . Y ) logger . info ( 'Solver time {:2.4f} seconds' . format ( self . last_invert_time )) logger . info ( 'Expected total time {:2.4f} minutes' . format ( ( self . ndraw_trace * ( 1 + self . compute_hetero ) + self . ndraw_pii * self . compute_hetero ) * self . last_invert_time / 60 )) self . E = self . Y - self . mult_A ( psi_hat , alpha_hat ) self . res [ 'solver_time' ] = self . last_invert_time fe_rsq = 1 - np . power ( self . E , 2 ) . mean () / np . power ( self . Y , 2 ) . mean () logger . info ( 'Fixed effect R-square {:2.4f} ' . format ( fe_rsq )) self . var_fe = np . var ( self . Jq * psi_hat ) self . cov_fe = np . cov ( self . Jq * psi_hat , self . Wq * alpha_hat )[ 0 ][ 1 ] self . tot_var = np . var ( self . Y ) logger . info ( '[fe] var_psi= {:2.4f} cov= {:2.4f} tot= {:2.4f} ' . format ( self . var_fe , self . cov_fe , self . tot_var )) self . var_e = self . nn / ( self . nn - self . nw - self . nf + 1 ) * np . power ( self . E , 2 ) . mean () logger . info ( '[ho] variance of residuals {:2.4f} ' . format ( self . var_e ))","title":"create_fe_solver()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.init_prepped_adata","text":"!!! purpose Use prepped adata to initialize class attributes Parameters: Name Type Description Default adata Pandas DataFrame labor data. Contains the following columns: wid (worker id) y1 (compensation 1) y2 (compensation 2) f1i (firm id 1) f2i (firm id 2) m (0 if stayer, 1 if mover) required Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def init_prepped_adata ( self ): ''' Purpose: Use prepped adata to initialize class attributes Arguments: adata (Pandas DataFrame): labor data. Contains the following columns: wid (worker id) y1 (compensation 1) y2 (compensation 2) f1i (firm id 1) f2i (firm id 2) m (0 if stayer, 1 if mover) Returns: Nothing ''' nf = self . adata . f1i . max () # Number of firms nw = self . adata . wid . max () # Number of workers nn = len ( self . adata ) # Number of observations self . nf = nf self . nw = nw self . nn = nn logger . info ( 'data nf: {} nw: {} nn: {} ' . format ( nf , nw , nn )) # Matrices for the cross-section J = csc_matrix (( np . ones ( nn ), ( self . adata . index , self . adata . f1i - 1 )), shape = ( nn , nf )) # Firms J = J [:, range ( nf - 1 )] # Normalize one firm to 0 self . J = J W = csc_matrix (( np . ones ( nn ), ( self . adata . index , self . adata . wid - 1 )), shape = ( nn , nw )) # Workers self . W = W # Dw = diags((W.T * W).diagonal()) # FIXME changed from .transpose() to .T ALSO commented this out since it's not used Dwinv = diags ( 1.0 / (( W . T * W ) . diagonal ())) # FIXME changed from .transpose() to .T self . Dwinv = Dwinv logger . info ( 'Prepare linear solver' ) # Finally create M M = J . T * J - J . T * W * Dwinv * W . T * J # FIXME changed from .transpose() to .T self . M = M self . ml = pyamg . ruge_stuben_solver ( M ) # L = diags(fes.M.diagonal()) - fes.M # r = linalg.eigsh(L,k=2,which='LM') # Create cross-section matrices # cs == 1 ==> looking at y1 for movers (cs = cross section) mdata = self . adata [ self . adata [ 'cs' ] == 1 ] # FIXME changed from adata.query('cs==1') (I ran %timeit and slicing is faster) mdata = mdata . reset_index ( drop = True ) # FIXME changed from set_index(pd.Series(range(len(mdata)))) nnq = len ( mdata ) # Number of observations self . nnq = nnq Jq = csc_matrix (( np . ones ( nnq ), ( mdata . index , mdata . f1i - 1 )), shape = ( nnq , nf )) self . Jq = Jq [:, range ( nf - 1 )] # normalizing one firm to 0 self . Wq = csc_matrix (( np . ones ( nnq ), ( mdata . index , mdata . wid - 1 )), shape = ( nnq , nw )) self . Yq = mdata [ 'y1' ] # Save time variable self . last_invert_time = 0","title":"init_prepped_adata()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.leverage_approx","text":"!!! purpose Compute an approximate leverage using ndraw_pii Parameters: Name Type Description Default ndraw_pii int number of draws required Returns: Type Description Pii (NumPy Array)","title":"leverage_approx()"},{"location":"fe-reference/#fixme-i-dont-know-what-this-function-does-so-i-dont-know-what-pii-is","text":"Source code in pytwoway/fe_approximate_correction_full.py def leverage_approx ( self , ndraw_pii ): ''' Purpose: Compute an approximate leverage using ndraw_pii Arguments: ndraw_pii (int): number of draws Returns: Pii (NumPy Array): # FIXME I don't know what this function does, so I don't know what Pii is ''' Pii = np . zeros ( self . nn ) # Compute the different draws for r in trange ( ndraw_pii ): R2 = 2 * np . random . binomial ( 1 , 0.5 , self . nn ) - 1 Pii += 1 / ndraw_pii * np . power ( self . proj ( R2 ), 2.0 ) logger . info ( 'Done with batch' ) return Pii","title":"FIXME I don't know what this function does, so I don't know what Pii is"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.load","text":"!!! purpose Load files for heteroskedastic correction Parameters: Name Type Description Default filename string file to load required Returns: Type Description fes loaded file Source code in pytwoway/fe_approximate_correction_full.py @staticmethod def load ( filename ): ''' Purpose: Load files for heteroskedastic correction Arguments: filename (string): file to load Returns: fes: loaded file ''' fes = None with open ( filename , 'rb' ) as infile : fes = pickle . load ( infile ) return fes","title":"load()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.mult_A","text":"!!! purpose Multiplies A = [J W] stored in the object by psi and alpha (used, for example, to compute estimated outcomes and sample errors) Parameters: Name Type Description Default psi firm fixed effects # FIXME correct datatype required alpha worker fixed effects # FIXME correct datatype required Returns: Type Description J_psi + W_alpha (CSC Matrix) firms * firm fixed effects + workers * worker fixed effects Source code in pytwoway/fe_approximate_correction_full.py def mult_A ( self , psi , alpha ): ''' Purpose: Multiplies A = [J W] stored in the object by psi and alpha (used, for example, to compute estimated outcomes and sample errors) Arguments: psi: firm fixed effects # FIXME correct datatype alpha: worker fixed effects # FIXME correct datatype Returns: J_psi + W_alpha (CSC Matrix): firms * firm fixed effects + workers * worker fixed effects ''' # J_psi = self.J * psi # W_alpha = self.W * alpha return self . J * psi + self . W * alpha # J_psi + W_alpha","title":"mult_A()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.mult_AAinv","text":"!!! purpose Multiplies gamma = [psi;alpha] by (A'A)^(-1) where A = [J W] stored in the object Parameters: Name Type Description Default psi firm fixed effects # FIXME correct datatype required alpha worker fixed effects # FIXME correct datatype required Returns: Type Description psi_out estimated firm fixed effects # FIXME correct datatype alpha_out : estimated worker fixed effects # FIXME correct datatype Source code in pytwoway/fe_approximate_correction_full.py def mult_AAinv ( self , psi , alpha ): ''' Purpose: Multiplies gamma = [psi;alpha] by (A'A)^(-1) where A = [J W] stored in the object Arguments: psi: firm fixed effects # FIXME correct datatype alpha: worker fixed effects # FIXME correct datatype Returns: psi_out: estimated firm fixed effects # FIXME correct datatype alpha_out : estimated worker fixed effects # FIXME correct datatype ''' # inter1 = self.ml.solve( psi , tol=1e-10 ) # inter2 = self.ml.solve( , tol=1e-10 ) # psi_out = inter1 - inter2 start = timer () psi_out = self . ml . solve ( psi - self . J . T * ( self . W * ( self . Dwinv * alpha )), tol = 1e-10 ) # FIXME changed from .transpose() to .T self . last_invert_time = timer () - start alpha_out = - self . Dwinv * ( self . W . T * ( self . J * psi_out )) + self . Dwinv * alpha # FIXME changed from .transpose() to .T return psi_out , alpha_out","title":"mult_AAinv()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.mult_Atranspose","text":"!!! purpose Multiplies the transpose of A = [J W] stored in the object by v Parameters: Name Type Description Default v what to multiply by # FIXME correct datatype required Returns: Type Description J_transpose_V (CSC Matrix) firms * v W_transpose_V (CSC Matrix): workers * v Source code in pytwoway/fe_approximate_correction_full.py def mult_Atranspose ( self , v ): ''' Purpose: Multiplies the transpose of A = [J W] stored in the object by v Arguments: v: what to multiply by # FIXME correct datatype Returns: J_transpose_V (CSC Matrix): firms * v W_transpose_V (CSC Matrix): workers * v ''' # J_transpose_V = self.J.T * v # FIXME changed from .transpose() to .T # W_transpose_V = self.W.T * v # FIXME changed from .transpose() to .T return self . J . T * v , self . W . T * v # J_transpose_V, W_transpose_V","title":"mult_Atranspose()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.prep_data","text":"!!! purpose Do some initial data cleaning Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def prep_data ( self ): ''' Purpose: Do some initial data cleaning Arguments: Nothing Returns: Nothing ''' logger . info ( 'Preparing the data' ) data = self . params [ 'data' ] sdata = data [ data [ 'm' ] == 0 ] . reset_index ( drop = True ) jdata = data [ data [ 'm' ] == 1 ] . reset_index ( drop = True ) logger . info ( 'Data movers= {} stayers= {} ' . format ( len ( jdata ), len ( sdata ))) self . res [ 'nm' ] = len ( jdata ) self . res [ 'ns' ] = len ( sdata ) self . res [ 'n_firms' ] = len ( np . unique ( pd . concat ([ jdata [ 'f1i' ], jdata [ 'f2i' ], sdata [ 'f1i' ]], ignore_index = True ))) self . res [ 'n_workers' ] = len ( np . unique ( pd . concat ([ jdata [ 'wid' ], sdata [ 'wid' ]], ignore_index = True ))) self . res [ 'n_movers' ] = len ( np . unique ( pd . concat ([ jdata [ 'wid' ]], ignore_index = True ))) #res['year_max'] = int(sdata['year'].max()) #res['year_min'] = int(sdata['year'].min()) # Make wids unique per row jdata . set_index ( np . arange ( self . res [ 'nm' ]) + 1 ) sdata . set_index ( np . arange ( self . res [ 'ns' ]) + 1 + self . res [ 'nm' ]) jdata [ 'wid' ] = np . arange ( self . res [ 'nm' ]) + 1 sdata [ 'wid' ] = np . arange ( self . res [ 'ns' ]) + 1 + self . res [ 'nm' ] # Combine the 2 data-sets self . adata = pd . concat ([ sdata [[ 'wid' , 'f1i' , 'y1' ]] . assign ( cs = 1 , m = 0 ), jdata [[ 'wid' , 'f1i' , 'y1' ]] . assign ( cs = 1 , m = 1 ), jdata [[ 'wid' , 'f2i' , 'y2' ]] . rename ( columns = { 'f2i' : 'f1i' , 'y2' : 'y1' }) . assign ( cs = 0 , m = 1 )]) self . adata = self . adata . reset_index ( drop = True ) # FIXME changed from set_index(pd.Series(range(len(self.adata)))) self . adata [ 'wid' ] = self . adata [ 'wid' ] . astype ( 'category' ) . cat . codes + 1","title":"prep_data()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.proj","text":"!!! purpose Solve y, then project onto X space of data stored in the object Parameters: Name Type Description Default y Pandas DataFrame labor data required Returns: Type Description Projection of psi, alpha solved from y onto X space Source code in pytwoway/fe_approximate_correction_full.py def proj ( self , y ): # FIXME should this y be Y? ''' Purpose: Solve y, then project onto X space of data stored in the object Arguments: y (Pandas DataFrame): labor data Returns: Projection of psi, alpha solved from y onto X space ''' return self . mult_A ( * self . solve ( y ))","title":"proj()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.save","text":"!!! purpose Save FEsolver class to filename as pickle Parameters: Name Type Description Default filename string filename to save to required Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def save ( self , filename ): ''' Purpose: Save FEsolver class to filename as pickle Arguments: filename (string): filename to save to Returns: Nothing ''' with open ( filename , 'wb' ) as outfile : pickle . dump ( self , outfile )","title":"save()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.save_early_stats","text":"!!! purpose Save the early statistics computed in compute_early_stats() Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def save_early_stats ( self ): ''' Purpose: Save the early statistics computed in compute_early_stats() Arguments: Nothing Returns: Nothing ''' with open ( self . params [ 'out' ], 'w' ) as outfile : json . dump ( self . res , outfile ) logger . info ( 'saved results to {} ' . format ( self . params [ 'out' ])) logger . info ( '--statsonly was passed as argument, so we skip all estimation.' ) logger . info ( '------ DONE -------' )","title":"save_early_stats()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.save_res","text":"!!! purpose Save results as json Returns: Type Description Nothing Source code in pytwoway/fe_approximate_correction_full.py def save_res ( self ): ''' Purpose: Save results as json Arguments: Nothing Returns: Nothing ''' with open ( self . params [ 'out' ], 'w' ) as outfile : json . dump ( self . res , outfile ) logger . info ( 'Saved results to {} ' . format ( self . params [ 'out' ]))","title":"save_res()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.solve","text":"!!! purpose Compute (A'A)^-1 A'Y, the least squares estimate of A [psi_hat, alpha_hat] = Y Parameters: Name Type Description Default Y Pandas DataFrame labor data required Returns: Type Description psi_hat estimated firm fixed effects # FIXME correct datatype alpha_hat: estimated worker fixed effects # FIXME correct datatype Source code in pytwoway/fe_approximate_correction_full.py def solve ( self , Y ): ''' Purpose: Compute (A'A)^-1 A'Y, the least squares estimate of A [psi_hat, alpha_hat] = Y Arguments: Y (Pandas DataFrame): labor data Returns: psi_hat: estimated firm fixed effects # FIXME correct datatype alpha_hat: estimated worker fixed effects # FIXME correct datatype ''' J_transpose_Y , W_transpose_Y = self . mult_Atranspose ( Y ) # This gives A'Y psi_hat , alpha_hat = self . mult_AAinv ( J_transpose_Y , W_transpose_Y ) return psi_hat , alpha_hat","title":"solve()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.weighted_quantile","text":"!!! purpose Very close to numpy.percentile, but supports weights. NOTE: quantiles should be in [0, 1]! Parameters: Name Type Description Default param values: numpy.array with data required param quantiles: array-like with many quantiles needed required param sample_weight: array-like of the same length as array required param values_sorted: bool, if True, then will avoid sorting of initial array required param old_style: if True, will correct output to be consistent with numpy.percentile. required Returns: Type Description return: numpy.array with computed quantiles. Source code in pytwoway/fe_approximate_correction_full.py def weighted_quantile ( self , values , quantiles , sample_weight = None , values_sorted = False , old_style = False ): # FIXME was formerly a function outside the class ''' Purpose: Very close to numpy.percentile, but supports weights. NOTE: quantiles should be in [0, 1]! Arguments: :param values: numpy.array with data :param quantiles: array-like with many quantiles needed :param sample_weight: array-like of the same length as `array` :param values_sorted: bool, if True, then will avoid sorting of initial array :param old_style: if True, will correct output to be consistent with numpy.percentile. Returns: :return: numpy.array with computed quantiles. ''' values = np . array ( values ) quantiles = np . array ( quantiles ) if sample_weight is None : sample_weight = np . ones ( len ( values )) sample_weight = np . array ( sample_weight ) assert np . all ( quantiles >= 0 ) and np . all ( quantiles <= 1 ), \\ 'quantiles should be in [0, 1]' if not values_sorted : sorter = np . argsort ( values ) values = values [ sorter ] sample_weight = sample_weight [ sorter ] weighted_quantiles = np . cumsum ( sample_weight ) - 0.5 * sample_weight if old_style : # To be convenient with numpy.percentile weighted_quantiles -= weighted_quantiles [ 0 ] weighted_quantiles /= weighted_quantiles [ - 1 ] else : weighted_quantiles /= np . sum ( sample_weight ) return np . interp ( quantiles , weighted_quantiles , values )","title":"weighted_quantile()"},{"location":"fe-reference/#fe_approximate_correction_full.FEsolver.weighted_var","text":"!!! purpose Compute weighted variance # FIXME I don't know what this function really does Parameters: Name Type Description Default v","title":"weighted_var()"},{"location":"fe-reference/#fixme-i-dont-know-what-this-is","text":"required w","title":"FIXME I don't know what this is"},{"location":"fe-reference/#fixme-i-dont-know-what-this-is","text":"required Returns: Type Description v0","title":"FIXME I don't know what this is"},{"location":"fe-reference/#fixme-i-dont-know-what-this-is","text":"Source code in pytwoway/fe_approximate_correction_full.py def weighted_var ( self , v , w ): # FIXME was formerly a function outside the class ''' Purpose: Compute weighted variance # FIXME I don't know what this function really does Arguments: v: # FIXME I don't know what this is w: # FIXME I don't know what this is Returns: v0: # FIXME I don't know what this is ''' m0 = np . sum ( w * v ) / np . sum ( w ) v0 = np . sum ( w * ( v - m0 ) ** 2 ) / np . sum ( w ) return v0","title":"FIXME I don't know what this is"},{"location":"twfe_network-reference/","text":"twfe_network module Class for a two-way fixed effect network twfe_network Class of twfe_network, where twfe_network gives a network of firms and workers. This class has the following functions: init (): initialize update_dict(): update values in parameter dictionaries (this function is similar to, but different from dict.update()) update_cols(): rename columns and keep only relevant columns n_workers(): get the number of unique workers n_firms(): get the number of unique firms data_validity(): check that data is formatted correctly conset(): update data to include only the largest connected set of movers, and if firm ids are contiguous, also return the NetworkX Graph contiguous_fids(): make firm ids contiguous refactor_es(): refactor long form data into event study data approx_cdfs(): generate cdfs of compensation for firms cluster(): cluster data and assign a new column giving the cluster for each firm run_akm_corrected(): run bias-corrected AKM estimator run_cre(): run CRE estimator sim_network_gen_fe(): generate fixed effects values for simulated panel data corresponding to the calibrated model (only for simulated data) sim_network_draw_fids(): draw firm ids for individual, given data that is grouped by worker id, spell id, and firm type (only for simulated data) sim_network(): simulate panel data corresponding to the calibrated model (only for simulated data) __init__ ( self , data = {}, formatting = 'long' , col_dict = False ) special !!! purpose Initialize twfe_network object Parameters: Name Type Description Default data dict or Pandas DataFrame if dict, simulate network of firms and workers using parameter values in dictionary; if Pandas DataFrame, then real data giving firms and workers Dictionary parameters: num_ind: number of workers num_time: time length of panel firm_size: max number of individuals per firm nk: number of firm types nl: number of worker types alpha_sig: standard error of individual fixed effect (volatility of worker effects) psi_sig: standard error of firm fixed effect (volatility of firm effects) w_sig: standard error of residual in AKM wage equation (volatility of wage shocks) csort: sorting effect cnetw: network effect csig: standard error of sorting/network effects p_move: probability a worker moves firms in any period {} formatting string if 'long', then data in long format; if 'es', then data in event study format. If simulating data, keep default value of 'long' 'long' col_dict dictionary make data columns readable (requires: wid (worker id), comp (compensation), fid (firm id), year if long; wid (worker id), y1 (compensation 1), y2 (compensation 2), f1i (firm id 1), f2i (firm id 2), m (0 if stayer, 1 if mover) if event study) False Returns: Type Description Object of type twfe_network Source code in pytwoway/twfe_network.py def __init__ ( self , data = {}, formatting = 'long' , col_dict = False ): ''' Purpose: Initialize twfe_network object Arguments: data (dict or Pandas DataFrame): if dict, simulate network of firms and workers using parameter values in dictionary; if Pandas DataFrame, then real data giving firms and workers Dictionary parameters: num_ind: number of workers num_time: time length of panel firm_size: max number of individuals per firm nk: number of firm types nl: number of worker types alpha_sig: standard error of individual fixed effect (volatility of worker effects) psi_sig: standard error of firm fixed effect (volatility of firm effects) w_sig: standard error of residual in AKM wage equation (volatility of wage shocks) csort: sorting effect cnetw: network effect csig: standard error of sorting/network effects p_move: probability a worker moves firms in any period formatting (string): if 'long', then data in long format; if 'es', then data in event study format. If simulating data, keep default value of 'long' col_dict (dictionary): make data columns readable (requires: wid (worker id), comp (compensation), fid (firm id), year if long; wid (worker id), y1 (compensation 1), y2 (compensation 2), f1i (firm id 1), f2i (firm id 2), m (0 if stayer, 1 if mover) if event study) Returns: Object of type twfe_network ''' logger . info ( 'initializing twfe_network object' ) # Define some variables self . connected = False self . contiguous = False self . formatting = formatting self . col_dict = col_dict # Define default parameter dictionaries self . default_data = { 'num_ind' : 10000 , 'num_time' : 5 , 'firm_size' : 50 , 'nk' : 10 , 'nl' : 5 , 'alpha_sig' : 1 , 'psi_sig' : 1 , 'w_sig' : 1 , 'csort' : 1 , 'cnetw' : 1 , 'csig' : 1 , 'p_move' : 0.5 } self . default_KMeans = { 'n_clusters' : 10 , 'init' : 'k-means++' , 'n_init' : 500 , 'max_iter' : 300 , 'tol' : 0.0001 , 'precompute_distances' : 'deprecated' , 'verbose' : 0 , 'random_state' : None , 'copy_x' : True , 'n_jobs' : 'deprecated' , 'algorithm' : 'auto' } self . default_akm = { 'ncore' : 1 , 'batch' : 1 , 'ndraw_pii' : 50 , 'ndraw_tr' : 5 , 'check' : False , 'hetero' : False , 'out' : 'res_akm.json' , 'con' : False , 'logfile' : '' , 'levfile' : '' , 'statsonly' : False } # Do not define 'data' because will be updated later self . default_cre = { 'ncore' : 1 , 'ndraw_tr' : 5 , 'ndp' : 50 , 'out' : 'res_cre.json' , 'posterior' : False , 'wobtw' : False } # Do not define 'data' because will be updated later # Simulate data if isinstance ( data , dict ): data = self . update_dict ( self . default_data , data ) self . data = self . sim_network ( data ) # Use given data else : self . data = data . dropna () # Make sure data is valid # Note that column names are corrected in this function if all columns are in the data self . data_validity () # Drop na values self . data = self . data . dropna () # Generate largest connected set self . conset () # Make firm ids contiguous self . contiguous_fids () # Using contiguous fids, get NetworkX Graph of largest connected set self . G = self . conset () # Check data validity after initial cleaning if isinstance ( col_dict , dict ): self . data_validity () approx_cdfs ( self , cdf_resolution = 10 , grouping = 'quantile_all' , year = None ) !!! purpose Generate cdfs of compensation for firms Parameters: Name Type Description Default cdf_resolution int how many values to use to approximate the cdf 10 grouping string how to group the cdfs ('quantile_all' to get quantiles from entire set of data, then have firm-level values between 0 and 1; 'quantile_firm_small' to get quantiles at the firm-level and have values be compensations if small data; 'quantile_firm_large' to get quantiles at the firm-level and have values be compensations if large data, note that this is up to 50 times slower than 'quantile_firm_small' and should only be used if the dataset is too large to copy into a dictionary) 'quantile_all' year int if None, uses entire dataset; if int, gives year of data to consider None Returns: Type Description cdf_df (numpy array) numpy array of firm cdfs Source code in pytwoway/twfe_network.py def approx_cdfs ( self , cdf_resolution = 10 , grouping = 'quantile_all' , year = None ): ''' Purpose: Generate cdfs of compensation for firms Arguments: cdf_resolution (int): how many values to use to approximate the cdf grouping (string): how to group the cdfs ('quantile_all' to get quantiles from entire set of data, then have firm-level values between 0 and 1; 'quantile_firm_small' to get quantiles at the firm-level and have values be compensations if small data; 'quantile_firm_large' to get quantiles at the firm-level and have values be compensations if large data, note that this is up to 50 times slower than 'quantile_firm_small' and should only be used if the dataset is too large to copy into a dictionary) year (int): if None, uses entire dataset; if int, gives year of data to consider Returns: cdf_df (numpy array): numpy array of firm cdfs ''' # If year-level, then only use data for that particular year if isinstance ( year , int ) and ( self . formatting == 'long' ): data = data [ data [ 'year' ] == year ] # Create empty numpy array to fill with the cdfs n_firms = self . n_firms () cdfs = np . zeros ([ n_firms , cdf_resolution ]) # Create quantiles of interest quantiles = np . linspace ( 1 / cdf_resolution , 1 , cdf_resolution ) # Re-arrange event study data to be in long format (temporarily) if self . formatting == 'es' : self . data = self . data . rename ({ 'f1i' : 'fid' , 'y1' : 'comp' }, axis = 1 ) self . data = pd . concat ([ self . data , self . data [[ 'f2i' , 'y2' ]] . rename ({ 'f2i' : 'fid' , 'y2' : 'comp' }, axis = 1 )], axis = 0 ) if grouping == 'quantile_all' : # Get quantiles from all data quantile_groups = self . data [ 'comp' ] . quantile ( quantiles ) # Generate firm-level cdfs for i , quant in enumerate ( quantile_groups ): cdfs [:, i ] = self . data . assign ( firm_quant = lambda d : d [ 'comp' ] <= quant ) . groupby ( 'fid' )[ 'firm_quant' ] . agg ( sum ) . to_numpy () # Normalize by firm size (convert to cdf) fsize = self . data . groupby ( 'fid' ) . size () . to_numpy () cdfs /= np . expand_dims ( fsize , 1 ) elif grouping in [ 'quantile_firm_small' , 'quantile_firm_large' ]: # Sort data by compensation (do this once now, so that don't need to do it again later) (also note it is faster to sort then manually compute quantiles than to use built-in quantile functions) self . data = self . data . sort_values ([ 'comp' ]) if grouping == 'quantile_firm_small' : # Convert pandas dataframe into a dictionary to access data faster # Source for idea: https://stackoverflow.com/questions/57208997/looking-for-the-fastest-way-to-slice-a-row-in-a-huge-pandas-dataframe # Source for how to actually format data correctly: https://stackoverflow.com/questions/56064677/pandas-series-to-dict-with-repeated-indices-make-dict-with-list-values data_dict = self . data [ 'comp' ] . groupby ( level = 0 ) . agg ( list ) . to_dict () # Generate the cdfs for row in tqdm ( range ( n_firms )): fid = row + 1 # fids start at 1 # Get the firm-level compensation data (don't need to sort because already sorted) if grouping == 'quantile_firm_small' : comp = data_dict [ fid ] elif grouping == 'quantile_firm_large' : comp = self . data . loc [ self . data [ 'fid' ] == fid , 'comp' ] # Generate the firm-level cdf # Note: update numpy array element by element # Source: https://stackoverflow.com/questions/30012362/faster-way-to-convert-list-of-objects-to-numpy-array/30012403 for i in range ( cdf_resolution ): index = max ( len ( comp ) * ( i + 1 ) // cdf_resolution - 1 , 0 ) # Don't want negative index # Update cdfs with the firm-level cdf cdfs [ row , i ] = comp [ index ] # Drop rows that were appended earlier and rename columns if self . formatting == 'es' : self . data = self . data . dropna () self . data = self . data . rename ({ 'fid' : 'f1i' , 'comp' : 'y1' }, axis = 1 ) return cdfs cluster ( self , cdf_resolution = 10 , grouping = 'quantile_all' , year = None , user_KMeans = {}) !!! purpose Cluster data and assign a new column giving the cluster for each firm Parameters: Name Type Description Default cdf_resolution int how many values to use to approximate the cdf 10 grouping string how to group the cdfs ('quantile_all' to get quantiles from entire set of data, then have firm-level values between 0 and 1; 'quantile_firm_small' to get quantiles at the firm-level and have values be compensations if small data; 'quantile_firm_large' to get quantiles at the firm-level and have values be compensations if large data, note that this is up to 50 times slower than 'quantile_firm_small' and should only be used if the dataset is too large to copy into a dictionary) 'quantile_all' year int if None, uses entire dataset; if int, gives year of data to consider None user_KMeans dict use parameters defined in KMeans_dict for KMeans estimation (for more information on what parameters can be used, visit https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), and use default parameters defined in class attribute default_KMeans for any parameters not specified {} Returns: Type Description Nothing Source code in pytwoway/twfe_network.py def cluster ( self , cdf_resolution = 10 , grouping = 'quantile_all' , year = None , user_KMeans = {}): ''' Purpose: Cluster data and assign a new column giving the cluster for each firm Arguments: cdf_resolution (int): how many values to use to approximate the cdf grouping (string): how to group the cdfs ('quantile_all' to get quantiles from entire set of data, then have firm-level values between 0 and 1; 'quantile_firm_small' to get quantiles at the firm-level and have values be compensations if small data; 'quantile_firm_large' to get quantiles at the firm-level and have values be compensations if large data, note that this is up to 50 times slower than 'quantile_firm_small' and should only be used if the dataset is too large to copy into a dictionary) year (int): if None, uses entire dataset; if int, gives year of data to consider user_KMeans (dict): use parameters defined in KMeans_dict for KMeans estimation (for more information on what parameters can be used, visit https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), and use default parameters defined in class attribute default_KMeans for any parameters not specified Returns: Nothing ''' if self . formatting == 'es' : # Compute cdfs cdfs = self . approx_cdfs ( cdf_resolution = cdf_resolution , grouping = grouping , year = year ) logger . info ( 'firm cdfs computed' ) # Compute firm clusters KMeans_params = self . update_dict ( self . default_KMeans , user_KMeans ) clusters = KMeans ( n_clusters = KMeans_params [ 'n_clusters' ], init = KMeans_params [ 'init' ], n_init = KMeans_params [ 'n_init' ], max_iter = KMeans_params [ 'max_iter' ], tol = KMeans_params [ 'tol' ], precompute_distances = KMeans_params [ 'precompute_distances' ], verbose = KMeans_params [ 'verbose' ], random_state = KMeans_params [ 'random_state' ], copy_x = KMeans_params [ 'copy_x' ], n_jobs = KMeans_params [ 'n_jobs' ], algorithm = KMeans_params [ 'algorithm' ]) . fit ( cdfs ) . labels_ + 1 # Need +1 because need > 0 logger . info ( 'firm clusters computed' ) # Create Pandas dataframe linking fid to firm cluster n_firms = cdfs . shape [ 0 ] fids = np . linspace ( 0 , n_firms - 1 , n_firms ) + 1 clusters_dict_1 = { 'f1i' : fids , 'j1' : clusters } clusters_dict_2 = { 'f2i' : fids , 'j2' : clusters } clusters_df_1 = pd . DataFrame ( clusters_dict_1 , index = fids ) clusters_df_2 = pd . DataFrame ( clusters_dict_2 , index = fids ) logger . info ( 'dataframes linked fids to clusters generated' ) # Merge into event study data self . data = self . data . merge ( clusters_df_1 , how = 'left' , on = 'f1i' ) self . data = self . data . merge ( clusters_df_2 , how = 'left' , on = 'f2i' ) logger . info ( 'clusters merged into event study data' ) # Correct datatypes self . data [[ 'f1i' , 'f2i' , 'm' ]] = self . data [[ 'f1i' , 'f2i' , 'm' ]] . astype ( int ) logger . info ( 'datatypes of clusters corrected' ) conset ( self ) !!! purpose Update data to include only the largest connected set of movers, and if firm ids are contiguous, also return the NetworkX Graph Returns: Type Description G (NetworkX Graph) largest connected set of movers (only returns if firm ids are contiguous, otherwise returns None) Source code in pytwoway/twfe_network.py def conset ( self ): ''' Purpose: Update data to include only the largest connected set of movers, and if firm ids are contiguous, also return the NetworkX Graph Arguments: Nothing Returns: G (NetworkX Graph): largest connected set of movers (only returns if firm ids are contiguous, otherwise returns None) ''' prev_workers = self . n_workers () if self . formatting == 'long' : # Add max firm id per worker to serve as a central node for the worker # self.data['fid_f1'] = self.data.groupby('wid')['fid'].transform(lambda a: a.shift(-1)) # FIXME - this is directed but is much slower self . data [ 'fid_max' ] = self . data . groupby ([ 'wid' ])[ 'fid' ] . transform ( max ) # FIXME - this is undirected but is much faster # Find largest connected set # Source: https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.components.connected_components.html G = nx . from_pandas_edgelist ( self . data , 'fid' , 'fid_max' ) # Drop fid_max self . data = self . data . drop ([ 'fid_max' ], axis = 1 ) # Update data if not connected if not self . connected : largest_cc = max ( nx . connected_components ( G ), key = len ) # Keep largest connected set of firms self . data = self . data [ self . data [ 'fid' ] . isin ( largest_cc )] elif self . formatting == 'es' : # Find largest connected set # Source: https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.components.connected_components.html G = nx . from_pandas_edgelist ( self . data , 'f1i' , 'f2i' ) # Update data if not connected if not self . connected : largest_cc = max ( nx . connected_components ( G ), key = len ) # Keep largest connected set of firms self . data = self . data [( self . data [ 'f1i' ] . isin ( largest_cc )) & ( self . data [ 'f2i' ] . isin ( largest_cc ))] # Data is now connected self . connected = True # If connected data != full data, set contiguous to False if prev_workers != self . n_workers (): self . contiguous = False # Return G if firm ids are contiguous (if they're not contiguous, they have to be updated first) if self . contiguous : return G return None contiguous_fids ( self ) !!! purpose Make firm ids contiguous Returns: Type Description Nothing Source code in pytwoway/twfe_network.py def contiguous_fids ( self ): ''' Purpose: Make firm ids contiguous Arguments: Nothing Returns: Nothing ''' # Generate fid_list (note that all columns listed in fid_list are included in the set of firm ids, and all columns are adjusted to have the new, contiguous firm ids) if self . formatting == 'long' : fid_list = [ 'fid' ] elif self . formatting == 'es' : fid_list = [ 'f1i' , 'f2i' ] # Create sorted set of unique fids fids = [] for fid in fid_list : fids += list ( self . data [ fid ] . unique ()) fids = sorted ( list ( set ( fids ))) # Create list of adjusted fids adjusted_fids = np . linspace ( 0 , len ( fids ) - 1 , len ( fids )) . astype ( int ) + 1 # Update each fid one at a time for fid in fid_list : # Create dictionary linking current to new fids, then convert into a dataframe for merging fids_dict = { fid : fids , 'adj_' + fid : adjusted_fids } fids_df = pd . DataFrame ( fids_dict , index = adjusted_fids ) # Merge new, contiguous fids into event study data self . data = self . data . merge ( fids_df , how = 'left' , on = fid ) # Drop old fid column and rename contiguous fid column self . data = self . data . drop ([ fid ], axis = 1 ) self . data = self . data . rename ({ 'adj_' + fid : fid }, axis = 1 ) # Firm ids are now contiguous self . contiguous = True data_validity ( self ) !!! purpose Check that data is formatted correctly. Results are logged Returns: Type Description Nothing Source code in pytwoway/twfe_network.py def data_validity ( self ): ''' Purpose: Check that data is formatted correctly. Results are logged Arguments: Nothing Returns: Nothing ''' if self . formatting == 'long' : success = True logger . info ( '--- checking columns ---' ) cols = True for col in [ 'wid' , 'comp' , 'fid' , 'year' ]: if self . col_dict [ col ] not in self . data . columns : logger . info ( col , 'missing from data' ) cols = False else : if col == 'year' : if self . data [ self . col_dict [ col ]] . dtype != 'int' : logger . info ( self . col_dict [ col ], 'has wrong dtype, should be int but is' , self . data [ self . col_dict [ col ]] . dtype ) cols = False elif col == 'comp' : if self . data [ self . col_dict [ col ]] . dtype != 'float64' : logger . info ( self . col_dict [ col ], 'has wrong dtype, should be float64 but is' , self . data [ self . col_dict [ col ]] . dtype ) cols = False logger . info ( 'columns correct:' + str ( cols )) if not cols : success = False raise ValueError ( 'Your data does not include the correct columns. The twfe_network object cannot be generated with your data.' ) else : # Correct column names self . update_cols () logger . info ( '--- checking worker-year observations ---' ) max_obs = self . data . groupby ([ 'wid' , 'year' ]) . size () . max () logger . info ( 'max number of worker-year observations (should be 1):' + str ( max_obs )) if max_obs > 1 : success = False logger . info ( '--- checking nan data ---' ) nan = self . data . shape [ 0 ] - self . data . dropna () . shape [ 0 ] logger . info ( 'data nan rows (should be 0):' + str ( nan )) if nan > 0 : success = False logger . info ( '--- checking connected set ---' ) self . data [ 'fid_max' ] = self . data . groupby ([ 'wid' ])[ 'fid' ] . transform ( max ) G = nx . from_pandas_edgelist ( self . data , 'fid' , 'fid_max' ) largest_cc = max ( nx . connected_components ( G ), key = len ) self . data = self . data . drop ([ 'fid_max' ], axis = 1 ) outside_cc = self . data [( ~ self . data [ 'fid' ] . isin ( largest_cc ))] . shape [ 0 ] logger . info ( 'observations outside connected set (should be 0):' + str ( outside_cc )) if outside_cc > 0 : success = False logger . info ( 'Overall success:' + str ( success )) elif self . formatting == 'es' : success_stayers = True success_movers = True logger . info ( '--- checking columns ---' ) cols = True for col in [ 'wid' , 'y1' , 'y2' , 'f1i' , 'f2i' , 'm' ]: if self . col_dict [ col ] not in self . data . columns : logger . info ( col , 'missing from stayers' ) cols = False else : if col in [ 'y1' , 'y2' ]: if self . data [ self . col_dict [ col ]] . dtype != 'float64' : logger . info ( col , 'has wrong dtype, should be float64 but is' , self . data [ self . col_dict [ col ]] . dtype ) cols = False elif col == 'm' : if self . data [ self . col_dict [ col ]] . dtype != 'int' : logger . info ( col , 'has wrong dtype, should be int but is' , self . data [ self . col_dict [ col ]] . dtype ) cols = False logger . info ( 'columns correct:' + str ( cols )) if not cols : success_stayers = False success_movers = False raise ValueError ( 'Your data does not include the correct columns. The twfe_network object cannot be generated with your data.' ) else : # Correct column names self . update_cols () stayers = self . data [ self . data [ 'm' ] == 0 ] movers = self . data [ self . data [ 'm' ] == 1 ] logger . info ( '--- checking rows ---' ) na_stayers = stayers . shape [ 0 ] - stayers . dropna () . shape [ 0 ] na_movers = movers . shape [ 0 ] - movers . dropna () . shape [ 0 ] logger . info ( 'stayers nan rows (should be 0):' + str ( na_stayers )) logger . info ( 'movers nan rows (should be 0):' + str ( na_movers )) if na_stayers > 0 : success_stayers = False if na_movers > 0 : success_movers = False logger . info ( '--- checking firms ---' ) firms_stayers = ( stayers [ 'f1i' ] != stayers [ 'f2i' ]) . sum () firms_movers = ( movers [ 'f1i' ] == movers [ 'f2i' ]) . sum () logger . info ( 'stayers with different firms (should be 0):' + str ( firms_stayers )) logger . info ( 'movers with same firm (should be 0):' + str ( firms_movers )) if firms_stayers > 0 : success_stayers = False if firms_movers > 0 : success_movers = False logger . info ( '--- checking income ---' ) income_stayers = ( stayers [ 'y1' ] != stayers [ 'y2' ]) . sum () logger . info ( 'stayers with different income (should be 0):' + str ( income_stayers )) if income_stayers > 0 : success_stayers = False logger . info ( '--- checking connected set ---' ) G = nx . from_pandas_edgelist ( movers , 'f1i' , 'f2i' ) largest_cc = max ( nx . connected_components ( G ), key = len ) cc_stayers = stayers [( ~ stayers [ 'f1i' ] . isin ( largest_cc )) | ( ~ stayers [ 'f2i' ] . isin ( largest_cc ))] . shape [ 0 ] cc_movers = movers [( ~ movers [ 'f1i' ] . isin ( largest_cc )) | ( ~ movers [ 'f2i' ] . isin ( largest_cc ))] . shape [ 0 ] logger . info ( 'stayers outside connected set (should be 0):' + str ( cc_stayers )) logger . info ( 'movers outside connected set (should be 0):' + str ( cc_movers )) if cc_stayers > 0 : success_stayers = False if cc_movers > 0 : success_movers = False logger . info ( 'Overall success for stayers:' + str ( success_stayers )) logger . info ( 'Overall success for movers:' + str ( success_movers )) n_firms ( self ) !!! purpose Get the number of unique firms Returns: Type Description (int) number of unique firms Source code in pytwoway/twfe_network.py def n_firms ( self ): ''' Purpose: Get the number of unique firms Arguments: Nothing Returns: (int): number of unique firms ''' if self . formatting == 'long' : return len ( self . data [ 'fid' ] . unique ()) elif self . formatting == 'es' : return len ( set ( list ( self . data [ 'f1i' ] . unique ()) + list ( self . data [ 'f2i' ] . unique ()))) n_workers ( self ) !!! purpose Get the number of unique workers Returns: Type Description (int) number of unique workers Source code in pytwoway/twfe_network.py def n_workers ( self ): ''' Purpose: Get the number of unique workers Arguments: Nothing Returns: (int): number of unique workers ''' return len ( self . data [ 'wid' ] . unique ()) refactor_es ( self ) !!! purpose Refactor long form data into event study data Returns: Type Description Nothing Source code in pytwoway/twfe_network.py def refactor_es ( self ): ''' Purpose: Refactor long form data into event study data Arguments: Nothing Returns: Nothing ''' if self . formatting == 'long' : # Sort data by wid and year self . data = self . data . sort_values ([ 'wid' , 'year' ]) logger . info ( 'data sorted by wid and year' ) # Introduce lagged fid and wid self . data [ 'fid_l1' ] = self . data [ 'fid' ] . shift ( periods = 1 ) self . data [ 'wid_l1' ] = self . data [ 'wid' ] . shift ( periods = 1 ) logger . info ( 'lagged fid introduced' ) # Generate spell ids # Source: https://stackoverflow.com/questions/59778744/pandas-grouping-and-aggregating-consecutive-rows-with-same-value-in-column new_spell = ( self . data [ 'fid' ] != self . data [ 'fid_l1' ]) | ( self . data [ 'wid' ] != self . data [ 'wid_l1' ]) # Allow for wid != wid_l1 to ensure that consecutive workers at the same firm get counted as different spells self . data [ 'spell_id' ] = new_spell . cumsum () logger . info ( 'spell ids generated' ) # Aggregate at the spell level spell = self . data . groupby ([ 'spell_id' ]) data_spell = spell . agg ( fid = pd . NamedAgg ( column = 'fid' , aggfunc = 'first' ), wid = pd . NamedAgg ( column = 'wid' , aggfunc = 'first' ), comp = pd . NamedAgg ( column = 'comp' , aggfunc = 'mean' ), year_start = pd . NamedAgg ( column = 'year' , aggfunc = 'min' ), year_end = pd . NamedAgg ( column = 'fid' , aggfunc = 'max' ) ) logger . info ( 'data aggregated at the spell level' ) ## Format as event study ## # Split workers by spell count spell_count = data_spell . groupby ([ 'wid' ]) . size () single_spell = spell_count [ spell_count == 1 ] . index stayers = data_spell [ data_spell [ 'wid' ] . isin ( single_spell )] mult_spell = spell_count [ spell_count > 1 ] . index movers = data_spell [ data_spell [ 'wid' ] . isin ( mult_spell )] logger . info ( 'workers split by spell count' ) # Add lagged values movers = movers . sort_values ([ 'wid' , 'year_start' ]) movers [ 'fid_l1' ] = movers [ 'fid' ] . shift ( periods = 1 ) movers [ 'wid_l1' ] = movers [ 'wid' ] . shift ( periods = 1 ) movers [ 'comp_l1' ] = movers [ 'comp' ] . shift ( periods = 1 ) movers = movers [ movers [ 'wid' ] == movers [ 'wid_l1' ]] # Update columns stayers = stayers . rename ({ 'fid' : 'f1i' , 'comp' : 'y1' }, axis = 1 ) stayers [ 'f2i' ] = stayers [ 'f1i' ] stayers [ 'y2' ] = stayers [ 'y1' ] stayers [ 'm' ] = 0 movers = movers . rename ({ 'fid_l1' : 'f1i' , 'fid' : 'f2i' , 'comp_l1' : 'y1' , 'comp' : 'y2' }, axis = 1 ) movers [ 'f1i' ] = movers [ 'f1i' ] . astype ( int ) movers [ 'm' ] = 1 # Keep only relevant columns stayers = stayers [[ 'wid' , 'y1' , 'y2' , 'f1i' , 'f2i' , 'm' ]] movers = movers [[ 'wid' , 'y1' , 'y2' , 'f1i' , 'f2i' , 'm' ]] logger . info ( 'columns updated' ) # Merge stayers and movers self . data = pd . concat ([ stayers , movers ]) # Update col_dict self . col_dict = { 'wid' : 'wid' , 'y1' : 'y1' , 'y2' : 'y2' , 'f1i' : 'f1i' , 'f2i' : 'f2i' , 'm' : 'm' } logger . info ( 'data reformatted as event study' ) # Data is now formatted as event study self . formatting = 'es' run_akm_corrected ( self , user_akm = {}) !!! purpose Run bias-corrected AKM estimator Parameters: Name Type Description Default user_akm dictionary dictionary of parameters for bias-corrected AKM estimation Dictionary parameters: ncore (int): number of cores to use batch (int): batch size to send in parallel ndraw_pii (int): number of draw to use in approximation for leverages ndraw_tr (int): number of draws to use in approximation for traces check (bool): whether to compute the non-approximated estimates as well hetero (bool): whether to compute the heteroskedastic estimates out (string): outputfile con (string): computes the smallest eigen values, this is the filepath where these results are saved logfile (string): log output to a logfile levfile (string): file to load precomputed leverages statsonly (bool): save data statistics only {} Returns: Type Description akm_res (dictionary) dictionary of results Source code in pytwoway/twfe_network.py def run_akm_corrected ( self , user_akm = {}): ''' Purpose: Run bias-corrected AKM estimator Arguments: user_akm (dictionary): dictionary of parameters for bias-corrected AKM estimation Dictionary parameters: ncore (int): number of cores to use batch (int): batch size to send in parallel ndraw_pii (int): number of draw to use in approximation for leverages ndraw_tr (int): number of draws to use in approximation for traces check (bool): whether to compute the non-approximated estimates as well hetero (bool): whether to compute the heteroskedastic estimates out (string): outputfile con (string): computes the smallest eigen values, this is the filepath where these results are saved logfile (string): log output to a logfile levfile (string): file to load precomputed leverages statsonly (bool): save data statistics only Returns: akm_res (dictionary): dictionary of results ''' akm_params = self . update_dict ( self . default_akm , user_akm ) akm_params [ 'data' ] = self . data # Make sure to use up-to-date data akm_res = feacf . FEsolver ( akm_params ) . res # feacf.main(akm_params) # FIXME corrected for new feacf file using class structure return akm_res run_cre ( self , user_cre = {}) !!! purpose Run CRE estimator Parameters: Name Type Description Default user_cre dictionary dictionary of parameters for CRE estimation Dictionary parameters: ncore (int): number of cores to use ndraw_tr (int): number of draws to use in approximation for traces ndp (int): number of draw to use in approximation for leverages out (string): outputfile posterior (bool): compute posterior variance wobtw (bool): sets between variation to 0, pure RE {} Returns: Type Description cre_res (dictionary) dictionary of results Source code in pytwoway/twfe_network.py def run_cre ( self , user_cre = {}): ''' Purpose: Run CRE estimator Arguments: user_cre (dictionary): dictionary of parameters for CRE estimation Dictionary parameters: ncore (int): number of cores to use ndraw_tr (int): number of draws to use in approximation for traces ndp (int): number of draw to use in approximation for leverages out (string): outputfile posterior (bool): compute posterior variance wobtw (bool): sets between variation to 0, pure RE Returns: cre_res (dictionary): dictionary of results ''' cre_params = self . update_dict ( self . default_cre , user_cre ) cre_params [ 'data' ] = self . data # Make sure to use up-to-date data cre_res = cre . main ( cre_params ) return cre_res sim_network ( self , params ) !!! purpose Simulate panel data corresponding to the calibrated model Parameters: Name Type Description Default params dictionary dictionary linking parameters to values Dictionary parameters: num_ind: number of workers num_time: time length of panel firm_size: max number of individuals per firm nk: number of firm types nl: number of worker types alpha_sig: standard error of individual fixed effect (volatility of worker effects) psi_sig: standard error of firm fixed effect (volatility of firm effects) w_sig: standard error of residual in AKM wage equation (volatility of wage shocks) csort: sorting effect cnetw: network effect csig: standard error of sorting/network effects p_move: probability a worker moves firms in any period required Returns: Type Description data (Pandas DataFrame) simulated network Source code in pytwoway/twfe_network.py def sim_network ( self , params ): ''' Purpose: Simulate panel data corresponding to the calibrated model Arguments: params (dictionary): dictionary linking parameters to values Dictionary parameters: num_ind: number of workers num_time: time length of panel firm_size: max number of individuals per firm nk: number of firm types nl: number of worker types alpha_sig: standard error of individual fixed effect (volatility of worker effects) psi_sig: standard error of firm fixed effect (volatility of firm effects) w_sig: standard error of residual in AKM wage equation (volatility of wage shocks) csort: sorting effect cnetw: network effect csig: standard error of sorting/network effects p_move: probability a worker moves firms in any period Returns: data (Pandas DataFrame): simulated network ''' # Generate fixed effects psi , alpha , G , H = self . sim_network_gen_fe ( params ) # Extract parameters num_ind , num_time , firm_size = params [ 'num_ind' ], params [ 'num_time' ], params [ 'firm_size' ] nk , nl , w_sig , p_move = params [ 'nk' ], params [ 'nl' ], params [ 'w_sig' ], params [ 'p_move' ] # Generate empty NumPy arrays network = np . zeros (( num_ind , num_time ), dtype = int ) spellcount = np . ones (( num_ind , num_time )) # Random draws of worker types for all individuals in panel sim_worker_types = randint ( low = 1 , high = nl , size = num_ind ) for i in range ( 0 , num_ind ): l = sim_worker_types [ i ] # At time 1, we draw from H for initial firm network [ i , 0 ] = choices ( range ( 0 , nk ), H [ l , :])[ 0 ] for t in range ( 1 , num_time ): # Hit moving shock if rand () < p_move : network [ i , t ] = choices ( range ( 0 , nk ), G [ l , network [ i , t - 1 ], :])[ 0 ] spellcount [ i , t ] = spellcount [ i , t - 1 ] + 1 else : network [ i , t ] = network [ i , t - 1 ] spellcount [ i , t ] = spellcount [ i , t - 1 ] # Compiling IDs and timestamps ids = np . reshape ( np . outer ( range ( 1 , num_ind + 1 ), np . ones ( num_time )), ( num_time * num_ind , 1 )) ids = ids . astype ( int )[:, 0 ] ts = np . reshape ( repmat ( range ( 1 , num_time + 1 ), num_ind , 1 ), ( num_time * num_ind , 1 )) ts = ts . astype ( int )[:, 0 ] # Compiling worker types types = np . reshape ( np . outer ( sim_worker_types , np . ones ( num_time )), ( num_time * num_ind , 1 )) alpha_data = alpha [ types . astype ( int )][:, 0 ] # Compiling firm types psi_data = psi [ np . reshape ( network , ( num_time * num_ind , 1 ))][:, 0 ] k_data = np . reshape ( network , ( num_time * num_ind , 1 ))[:, 0 ] # Compiling spell data spell_data = np . reshape ( spellcount , ( num_time * num_ind , 1 ))[:, 0 ] # Merging all columns into a dataframe data = pd . DataFrame ( data = { 'wid' : ids , 'year' : ts , 'k' : k_data , 'alpha' : alpha_data , 'psi' : psi_data , 'spell' : spell_data . astype ( int )}) # Generate size of spells dspell = data . groupby ([ 'wid' , 'spell' , 'k' ]) . size () . to_frame ( name = 'freq' ) . reset_index () # Draw firm ids dspell [ 'fid' ] = dspell . groupby ([ 'k' ])[ 'freq' ] . transform ( self . sim_network_draw_fids , * [ num_time , firm_size ]) # Make firm ids contiguous (and have them start at 1) dspell [ 'fid' ] = dspell . groupby ([ 'k' , 'fid' ])[ 'freq' ] . ngroup () + 1 # Merge spells into panel data = data . merge ( dspell , on = [ 'wid' , 'spell' , 'k' ]) data [ 'move' ] = ( data [ 'fid' ] != data [ 'fid' ] . shift ( 1 )) & ( data [ 'wid' ] == data [ 'wid' ] . shift ( 1 )) # Compute wages through the AKM formula data [ 'comp' ] = data [ 'alpha' ] + data [ 'psi' ] + w_sig * norm . rvs ( size = num_ind * num_time ) return data sim_network_draw_fids ( self , freq , num_time , firm_size ) !!! purpose Draw firm ids for individual, given data that is grouped by worker id, spell id, and firm type Parameters: Name Type Description Default freq NumPy array size of groups (groups by worker id, spell id, and firm type) required num_time int time length of panel required firm_size int max number of individuals per firm required Returns: Type Description (NumPy array) random firms for each group Source code in pytwoway/twfe_network.py def sim_network_draw_fids ( self , freq , num_time , firm_size ): ''' Purpose: Draw firm ids for individual, given data that is grouped by worker id, spell id, and firm type Arguments: freq (NumPy array): size of groups (groups by worker id, spell id, and firm type) num_time (int): time length of panel firm_size (int): max number of individuals per firm Returns: (NumPy array): random firms for each group ''' max_int = np . int ( np . maximum ( 1 , freq . sum () / ( firm_size * num_time ))) return np . array ( np . random . choice ( max_int , size = freq . count ()) + 1 ) sim_network_gen_fe ( self , params ) !!! purpose Generate fixed effects values for simulated panel data corresponding to the calibrated model Parameters: Name Type Description Default params dictionary dictionary linking parameters to values Dictionary parameters: num_ind: number of workers num_time: time length of panel firm_size: max number of individuals per firm nk: number of firm types nl: number of worker types alpha_sig: standard error of individual fixed effect (volatility of worker effects) psi_sig: standard error of firm fixed effect (volatility of firm effects) w_sig: standard error of residual in AKM wage equation (volatility of wage shocks) csort: sorting effect cnetw: network effect csig: standard error of sorting/network effects p_move: probability a worker moves firms in any period required Returns: Type Description psi (NumPy array) array of firm fixed effects alpha (NumPy array): array of individual fixed effects G (NumPy array): transition matrices H (NumPy array): stationary distribution Source code in pytwoway/twfe_network.py def sim_network_gen_fe ( self , params ): ''' Purpose: Generate fixed effects values for simulated panel data corresponding to the calibrated model Arguments: params (dictionary): dictionary linking parameters to values Dictionary parameters: num_ind: number of workers num_time: time length of panel firm_size: max number of individuals per firm nk: number of firm types nl: number of worker types alpha_sig: standard error of individual fixed effect (volatility of worker effects) psi_sig: standard error of firm fixed effect (volatility of firm effects) w_sig: standard error of residual in AKM wage equation (volatility of wage shocks) csort: sorting effect cnetw: network effect csig: standard error of sorting/network effects p_move: probability a worker moves firms in any period Returns: psi (NumPy array): array of firm fixed effects alpha (NumPy array): array of individual fixed effects G (NumPy array): transition matrices H (NumPy array): stationary distribution ''' # Extract parameters nk , nl , alpha_sig , psi_sig = params [ 'nk' ], params [ 'nl' ], params [ 'alpha_sig' ], params [ 'psi_sig' ] csort , cnetw , csig = params [ 'csort' ], params [ 'cnetw' ], params [ 'csig' ] # Draw fixed effects psi = norm . ppf ( np . linspace ( 1 , nk , nk ) / ( nk + 1 )) * psi_sig alpha = norm . ppf ( np . linspace ( 1 , nl , nl ) / ( nl + 1 )) * alpha_sig # Generate transition matrices G = norm . pdf (( psi [ ax , ax , :] - cnetw * psi [ ax , :, ax ] - csort * alpha [:, ax , ax ]) / csig ) G = np . divide ( G , G . sum ( axis = 2 )[:, :, ax ]) # Generate empty stationary distributions H = np . ones (( nl , nk )) / nl # Solve stationary distributions for l in range ( 0 , nl ): # Solve eigenvectors # Source: https://stackoverflow.com/questions/31791728/python-code-explanation-for-stationary-distribution-of-a-markov-chain S , U = eig ( G [ l , :, :] . T ) stationary = np . array ( U [:, np . where ( np . abs ( S - 1. ) < 1e-8 )[ 0 ][ 0 ]] . flat ) stationary = stationary / np . sum ( stationary ) H [ l , :] = stationary return psi , alpha , G , H update_cols ( self ) !!! purpose Rename columns and keep only relevant columns Returns: Type Description Nothing Source code in pytwoway/twfe_network.py def update_cols ( self ): ''' Purpose: Rename columns and keep only relevant columns Arguments: Nothing Returns: Nothing ''' if isinstance ( self . col_dict , dict ): if self . formatting == 'long' : self . data = self . data . rename ({ self . col_dict [ 'wid' ]: 'wid' , self . col_dict [ 'comp' ]: 'comp' , self . col_dict [ 'fid' ]: 'fid' , self . col_dict [ 'year' ]: 'year' }, axis = 1 ) self . data = self . data [[ 'wid' , 'comp' , 'fid' , 'year' ]] self . col_dict = { 'wid' : 'wid' , 'comp' : 'comp' , 'fid' : 'fid' , 'year' : 'year' } elif self . formatting == 'es' : self . data = self . data . rename ({ self . col_dict [ 'wid' ]: 'wid' , self . col_dict [ 'y1' ]: 'y1' , self . col_dict [ 'y2' ]: 'y2' , self . col_dict [ 'f1i' ]: 'f1i' , self . col_dict [ 'f2i' ]: 'f2i' , self . col_dict [ 'm' ]: 'm' }, axis = 1 ) self . data = self . data [[ 'wid' , 'y1' , 'y2' , 'f1i' , 'f2i' , 'm' ]] self . col_dict = { 'wid' : 'wid' , 'y1' : 'y1' , 'y2' : 'y2' , 'f1i' : 'f1i' , 'f2i' : 'f2i' , 'm' : 'm' } update_dict ( self , default_params , user_params ) !!! purpose Replace entries in default_params with values in user_params. This function allows user_params to include only a subset of the required parameters in the dictionary Parameters: Name Type Description Default default_params dict default parameter values required user_params dict user selected parameter values required Returns: Type Description params (dict) default_params updated with parameter values in user_params Source code in pytwoway/twfe_network.py def update_dict ( self , default_params , user_params ): ''' Purpose: Replace entries in default_params with values in user_params. This function allows user_params to include only a subset of the required parameters in the dictionary Arguments: default_params (dict): default parameter values user_params (dict): user selected parameter values Returns: params (dict): default_params updated with parameter values in user_params ''' params = default_params . copy () params . update ( user_params ) return params twfe_monte_carlo ( N = 500 , ncore = 1 , data_params = {}, akm_params = {}, cre_params = {}, cdf_resolution = 10 , grouping = 'quantile_all' , year = None , KMeans_params = {}) !!! purpose Run Monte Carlo simulations of twfe_network to see the distribution of the true vs. estimated variance of psi and covariance between psi and alpha Parameters: Name Type Description Default N int number of simulations 500 ncore int how many cores to use 1 Other parameters see twfe_monte_carlo_interior() required Returns: Type Description true_psi_var (NumPy array) true simulated sample variance of psi true_psi_alpha_cov (NumPy array): true simulated sample covariance of psi and alpha akm_psi_var (NumPy array): AKM estimate of variance of psi akm_psi_alpha_cov (NumPy array): AKM estimate of covariance of psi and alpha akm_corr_psi_var (NumPy array): bias-corrected AKM estimate of variance of psi akm_corr_psi_alpha_cov (NumPy array): bias-corrected AKM estimate of covariance of psi and alpha cre_psi_var (NumPy array): CRE estimate of variance of psi cre_psi_alpha_cov (NumPy array): CRE estimate of covariance of psi and alpha Source code in pytwoway/twfe_network.py def twfe_monte_carlo ( N = 500 , ncore = 1 , data_params = {}, akm_params = {}, cre_params = {}, cdf_resolution = 10 , grouping = 'quantile_all' , year = None , KMeans_params = {}): ''' Purpose: Run Monte Carlo simulations of twfe_network to see the distribution of the true vs. estimated variance of psi and covariance between psi and alpha Arguments: N (int): number of simulations ncore (int): how many cores to use Other parameters: see twfe_monte_carlo_interior() Returns: true_psi_var (NumPy array): true simulated sample variance of psi true_psi_alpha_cov (NumPy array): true simulated sample covariance of psi and alpha akm_psi_var (NumPy array): AKM estimate of variance of psi akm_psi_alpha_cov (NumPy array): AKM estimate of covariance of psi and alpha akm_corr_psi_var (NumPy array): bias-corrected AKM estimate of variance of psi akm_corr_psi_alpha_cov (NumPy array): bias-corrected AKM estimate of covariance of psi and alpha cre_psi_var (NumPy array): CRE estimate of variance of psi cre_psi_alpha_cov (NumPy array): CRE estimate of covariance of psi and alpha ''' # Initialize NumPy arrays to store results true_psi_var = np . zeros ( N ) true_psi_alpha_cov = np . zeros ( N ) akm_psi_var = np . zeros ( N ) akm_psi_alpha_cov = np . zeros ( N ) akm_corr_psi_var = np . zeros ( N ) akm_corr_psi_alpha_cov = np . zeros ( N ) cre_psi_var = np . zeros ( N ) cre_psi_alpha_cov = np . zeros ( N ) # Use multi-processing if ncore > 1 : V = [] # Simulate networks with Pool ( processes = ncore ) as pool : V = pool . starmap ( twfe_monte_carlo_interior , [[ data_params , akm_params , cre_params , cdf_resolution , grouping , year , KMeans_params ] for _ in range ( N )]) for i , res in enumerate ( V ): true_psi_var [ i ], true_psi_alpha_cov [ i ], akm_psi_var [ i ], akm_psi_alpha_cov [ i ], akm_corr_psi_var [ i ], akm_corr_psi_alpha_cov [ i ], cre_psi_var [ i ], cre_psi_alpha_cov [ i ] = res else : for i in range ( N ): # Simulate a network true_psi_var [ i ], true_psi_alpha_cov [ i ], akm_psi_var [ i ], akm_psi_alpha_cov [ i ], akm_corr_psi_var [ i ], akm_corr_psi_alpha_cov [ i ], cre_psi_var [ i ], cre_psi_alpha_cov [ i ] = twfe_monte_carlo_interior ( data_params = data_params , akm_params = akm_params , cre_params = cre_params , cdf_resolution = cdf_resolution , grouping = grouping , year = year , KMeans_params = KMeans_params ) return true_psi_var , true_psi_alpha_cov , akm_psi_var , akm_psi_alpha_cov , akm_corr_psi_var , akm_corr_psi_alpha_cov , cre_psi_var , cre_psi_alpha_cov twfe_monte_carlo_interior ( data_params = {}, akm_params = {}, cre_params = {}, cdf_resolution = 10 , grouping = 'quantile_all' , year = None , KMeans_params = {}) !!! purpose Run Monte Carlo simulations of twfe_network to see the distribution of the true vs. estimated variance of psi and covariance between psi and alpha. This is the interior function to twfe_monte_carlo Parameters: Name Type Description Default data_params dictionary parameters for simulated data Dictionary parameters: num_ind: number of workers num_time: time length of panel firm_size: max number of individuals per firm nk: number of firm types nl: number of worker types alpha_sig: standard error of individual fixed effect (volatility of worker effects) psi_sig: standard error of firm fixed effect (volatility of firm effects) w_sig: standard error of residual in AKM wage equation (volatility of wage shocks) csort: sorting effect cnetw: network effect csig: standard error of sorting/network effects p_move: probability a worker moves firms in any period {} akm_params dictionary dictionary of parameters for bias-corrected AKM estimation Dictionary parameters: ncore (int): number of cores to use batch (int): batch size to send in parallel ndraw_pii (int): number of draw to use in approximation for leverages ndraw_tr (int): number of draws to use in approximation for traces check (bool): whether to compute the non-approximated estimates as well hetero (bool): whether to compute the heteroskedastic estimates out (string): outputfile con (string): computes the smallest eigen values, this is the filepath where these results are saved logfile (string): log output to a logfile levfile (string): file to load precomputed leverages statsonly (bool): save data statistics only {} cre_params dictionary dictionary of parameters for CRE estimation Dictionary parameters: ncore (int): number of cores to use ndraw_tr (int): number of draws to use in approximation for traces ndp (int): number of draw to use in approximation for leverages out (string): outputfile posterior (bool): compute posterior variance wobtw (bool): sets between variation to 0, pure RE {} Used for clustering cdf_resolution (int): how many values to use to approximate the cdf grouping (string): how to group the cdfs ('quantile_all' to get quantiles from entire set of data, then have firm-level values between 0 and 1; 'quantile_firm_small' to get quantiles at the firm-level and have values be compensations if small data; 'quantile_firm_large' to get quantiles at the firm-level and have values be compensations if large data, note that this is up to 50 times slower than 'quantile_firm_small' and should only be used if the dataset is too large to copy into a dictionary) year (int): if None, uses entire dataset; if int, gives year of data to consider KMeans_params (dict): use parameters defined in KMeans_dict for KMeans estimation (for more information on what parameters can be used, visit https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), and use default parameters defined in class attribute default_KMeans for any parameters not specified required Returns: Type Description true_psi_var (float) true simulated sample variance of psi true_psi_alpha_cov (float): true simulated sample covariance of psi and alpha akm_psi_var (float): AKM estimate of variance of psi akm_psi_alpha_cov (float): AKM estimate of covariance of psi and alpha akm_corr_psi_var (float): bias-corrected AKM estimate of variance of psi akm_corr_psi_alpha_cov (float): bias-corrected AKM estimate of covariance of psi and alpha cre_psi_var (float): CRE estimate of variance of psi cre_psi_alpha_cov (float): CRE estimate of covariance of psi and alpha Source code in pytwoway/twfe_network.py def twfe_monte_carlo_interior ( data_params = {}, akm_params = {}, cre_params = {}, cdf_resolution = 10 , grouping = 'quantile_all' , year = None , KMeans_params = {}): ''' Purpose: Run Monte Carlo simulations of twfe_network to see the distribution of the true vs. estimated variance of psi and covariance between psi and alpha. This is the interior function to twfe_monte_carlo Arguments: data_params (dictionary): parameters for simulated data Dictionary parameters: num_ind: number of workers num_time: time length of panel firm_size: max number of individuals per firm nk: number of firm types nl: number of worker types alpha_sig: standard error of individual fixed effect (volatility of worker effects) psi_sig: standard error of firm fixed effect (volatility of firm effects) w_sig: standard error of residual in AKM wage equation (volatility of wage shocks) csort: sorting effect cnetw: network effect csig: standard error of sorting/network effects p_move: probability a worker moves firms in any period akm_params (dictionary): dictionary of parameters for bias-corrected AKM estimation Dictionary parameters: ncore (int): number of cores to use batch (int): batch size to send in parallel ndraw_pii (int): number of draw to use in approximation for leverages ndraw_tr (int): number of draws to use in approximation for traces check (bool): whether to compute the non-approximated estimates as well hetero (bool): whether to compute the heteroskedastic estimates out (string): outputfile con (string): computes the smallest eigen values, this is the filepath where these results are saved logfile (string): log output to a logfile levfile (string): file to load precomputed leverages statsonly (bool): save data statistics only cre_params (dictionary): dictionary of parameters for CRE estimation Dictionary parameters: ncore (int): number of cores to use ndraw_tr (int): number of draws to use in approximation for traces ndp (int): number of draw to use in approximation for leverages out (string): outputfile posterior (bool): compute posterior variance wobtw (bool): sets between variation to 0, pure RE Used for clustering: cdf_resolution (int): how many values to use to approximate the cdf grouping (string): how to group the cdfs ('quantile_all' to get quantiles from entire set of data, then have firm-level values between 0 and 1; 'quantile_firm_small' to get quantiles at the firm-level and have values be compensations if small data; 'quantile_firm_large' to get quantiles at the firm-level and have values be compensations if large data, note that this is up to 50 times slower than 'quantile_firm_small' and should only be used if the dataset is too large to copy into a dictionary) year (int): if None, uses entire dataset; if int, gives year of data to consider KMeans_params (dict): use parameters defined in KMeans_dict for KMeans estimation (for more information on what parameters can be used, visit https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), and use default parameters defined in class attribute default_KMeans for any parameters not specified Returns: true_psi_var (float): true simulated sample variance of psi true_psi_alpha_cov (float): true simulated sample covariance of psi and alpha akm_psi_var (float): AKM estimate of variance of psi akm_psi_alpha_cov (float): AKM estimate of covariance of psi and alpha akm_corr_psi_var (float): bias-corrected AKM estimate of variance of psi akm_corr_psi_alpha_cov (float): bias-corrected AKM estimate of covariance of psi and alpha cre_psi_var (float): CRE estimate of variance of psi cre_psi_alpha_cov (float): CRE estimate of covariance of psi and alpha ''' # Simulate network nw = twfe_network ( data = data_params ) # Compute true sample variance of psi and covariance of psi and alpha psi_var = np . var ( nw . data [ 'psi' ]) psi_alpha_cov = np . cov ( nw . data [ 'psi' ], nw . data [ 'alpha' ])[ 0 , 1 ] # Convert into event study nw . refactor_es () # Estimate AKM model akm_res = nw . run_akm_corrected ( user_akm = akm_params ) # Cluster for CRE model nw . cluster ( cdf_resolution = cdf_resolution , grouping = grouping , year = year , user_KMeans = KMeans_params ) # Estimate CRE model cre_res = nw . run_cre ( user_cre = cre_params ) return psi_var , psi_alpha_cov , \\ akm_res [ 'var_fe' ], akm_res [ 'cov_fe' ], \\ akm_res [ 'var_ho' ], akm_res [ 'cov_ho' ], \\ cre_res [ 'var_wt' ] + cre_res [ 'var_bw' ], cre_res [ 'cov_wt' ] + cre_res [ 'cov_bw' ]","title":"twfe_network"},{"location":"twfe_network-reference/#twfe_network-module","text":"","title":"twfe_network module"},{"location":"twfe_network-reference/#twfe_network","text":"Class for a two-way fixed effect network","title":"twfe_network"},{"location":"twfe_network-reference/#twfe_network.twfe_network","text":"Class of twfe_network, where twfe_network gives a network of firms and workers. This class has the following functions: init (): initialize update_dict(): update values in parameter dictionaries (this function is similar to, but different from dict.update()) update_cols(): rename columns and keep only relevant columns n_workers(): get the number of unique workers n_firms(): get the number of unique firms data_validity(): check that data is formatted correctly conset(): update data to include only the largest connected set of movers, and if firm ids are contiguous, also return the NetworkX Graph contiguous_fids(): make firm ids contiguous refactor_es(): refactor long form data into event study data approx_cdfs(): generate cdfs of compensation for firms cluster(): cluster data and assign a new column giving the cluster for each firm run_akm_corrected(): run bias-corrected AKM estimator run_cre(): run CRE estimator sim_network_gen_fe(): generate fixed effects values for simulated panel data corresponding to the calibrated model (only for simulated data) sim_network_draw_fids(): draw firm ids for individual, given data that is grouped by worker id, spell id, and firm type (only for simulated data) sim_network(): simulate panel data corresponding to the calibrated model (only for simulated data)","title":"twfe_network"},{"location":"twfe_network-reference/#twfe_network.twfe_network.__init__","text":"!!! purpose Initialize twfe_network object Parameters: Name Type Description Default data dict or Pandas DataFrame if dict, simulate network of firms and workers using parameter values in dictionary; if Pandas DataFrame, then real data giving firms and workers Dictionary parameters: num_ind: number of workers num_time: time length of panel firm_size: max number of individuals per firm nk: number of firm types nl: number of worker types alpha_sig: standard error of individual fixed effect (volatility of worker effects) psi_sig: standard error of firm fixed effect (volatility of firm effects) w_sig: standard error of residual in AKM wage equation (volatility of wage shocks) csort: sorting effect cnetw: network effect csig: standard error of sorting/network effects p_move: probability a worker moves firms in any period {} formatting string if 'long', then data in long format; if 'es', then data in event study format. If simulating data, keep default value of 'long' 'long' col_dict dictionary make data columns readable (requires: wid (worker id), comp (compensation), fid (firm id), year if long; wid (worker id), y1 (compensation 1), y2 (compensation 2), f1i (firm id 1), f2i (firm id 2), m (0 if stayer, 1 if mover) if event study) False Returns: Type Description Object of type twfe_network Source code in pytwoway/twfe_network.py def __init__ ( self , data = {}, formatting = 'long' , col_dict = False ): ''' Purpose: Initialize twfe_network object Arguments: data (dict or Pandas DataFrame): if dict, simulate network of firms and workers using parameter values in dictionary; if Pandas DataFrame, then real data giving firms and workers Dictionary parameters: num_ind: number of workers num_time: time length of panel firm_size: max number of individuals per firm nk: number of firm types nl: number of worker types alpha_sig: standard error of individual fixed effect (volatility of worker effects) psi_sig: standard error of firm fixed effect (volatility of firm effects) w_sig: standard error of residual in AKM wage equation (volatility of wage shocks) csort: sorting effect cnetw: network effect csig: standard error of sorting/network effects p_move: probability a worker moves firms in any period formatting (string): if 'long', then data in long format; if 'es', then data in event study format. If simulating data, keep default value of 'long' col_dict (dictionary): make data columns readable (requires: wid (worker id), comp (compensation), fid (firm id), year if long; wid (worker id), y1 (compensation 1), y2 (compensation 2), f1i (firm id 1), f2i (firm id 2), m (0 if stayer, 1 if mover) if event study) Returns: Object of type twfe_network ''' logger . info ( 'initializing twfe_network object' ) # Define some variables self . connected = False self . contiguous = False self . formatting = formatting self . col_dict = col_dict # Define default parameter dictionaries self . default_data = { 'num_ind' : 10000 , 'num_time' : 5 , 'firm_size' : 50 , 'nk' : 10 , 'nl' : 5 , 'alpha_sig' : 1 , 'psi_sig' : 1 , 'w_sig' : 1 , 'csort' : 1 , 'cnetw' : 1 , 'csig' : 1 , 'p_move' : 0.5 } self . default_KMeans = { 'n_clusters' : 10 , 'init' : 'k-means++' , 'n_init' : 500 , 'max_iter' : 300 , 'tol' : 0.0001 , 'precompute_distances' : 'deprecated' , 'verbose' : 0 , 'random_state' : None , 'copy_x' : True , 'n_jobs' : 'deprecated' , 'algorithm' : 'auto' } self . default_akm = { 'ncore' : 1 , 'batch' : 1 , 'ndraw_pii' : 50 , 'ndraw_tr' : 5 , 'check' : False , 'hetero' : False , 'out' : 'res_akm.json' , 'con' : False , 'logfile' : '' , 'levfile' : '' , 'statsonly' : False } # Do not define 'data' because will be updated later self . default_cre = { 'ncore' : 1 , 'ndraw_tr' : 5 , 'ndp' : 50 , 'out' : 'res_cre.json' , 'posterior' : False , 'wobtw' : False } # Do not define 'data' because will be updated later # Simulate data if isinstance ( data , dict ): data = self . update_dict ( self . default_data , data ) self . data = self . sim_network ( data ) # Use given data else : self . data = data . dropna () # Make sure data is valid # Note that column names are corrected in this function if all columns are in the data self . data_validity () # Drop na values self . data = self . data . dropna () # Generate largest connected set self . conset () # Make firm ids contiguous self . contiguous_fids () # Using contiguous fids, get NetworkX Graph of largest connected set self . G = self . conset () # Check data validity after initial cleaning if isinstance ( col_dict , dict ): self . data_validity ()","title":"__init__()"},{"location":"twfe_network-reference/#twfe_network.twfe_network.approx_cdfs","text":"!!! purpose Generate cdfs of compensation for firms Parameters: Name Type Description Default cdf_resolution int how many values to use to approximate the cdf 10 grouping string how to group the cdfs ('quantile_all' to get quantiles from entire set of data, then have firm-level values between 0 and 1; 'quantile_firm_small' to get quantiles at the firm-level and have values be compensations if small data; 'quantile_firm_large' to get quantiles at the firm-level and have values be compensations if large data, note that this is up to 50 times slower than 'quantile_firm_small' and should only be used if the dataset is too large to copy into a dictionary) 'quantile_all' year int if None, uses entire dataset; if int, gives year of data to consider None Returns: Type Description cdf_df (numpy array) numpy array of firm cdfs Source code in pytwoway/twfe_network.py def approx_cdfs ( self , cdf_resolution = 10 , grouping = 'quantile_all' , year = None ): ''' Purpose: Generate cdfs of compensation for firms Arguments: cdf_resolution (int): how many values to use to approximate the cdf grouping (string): how to group the cdfs ('quantile_all' to get quantiles from entire set of data, then have firm-level values between 0 and 1; 'quantile_firm_small' to get quantiles at the firm-level and have values be compensations if small data; 'quantile_firm_large' to get quantiles at the firm-level and have values be compensations if large data, note that this is up to 50 times slower than 'quantile_firm_small' and should only be used if the dataset is too large to copy into a dictionary) year (int): if None, uses entire dataset; if int, gives year of data to consider Returns: cdf_df (numpy array): numpy array of firm cdfs ''' # If year-level, then only use data for that particular year if isinstance ( year , int ) and ( self . formatting == 'long' ): data = data [ data [ 'year' ] == year ] # Create empty numpy array to fill with the cdfs n_firms = self . n_firms () cdfs = np . zeros ([ n_firms , cdf_resolution ]) # Create quantiles of interest quantiles = np . linspace ( 1 / cdf_resolution , 1 , cdf_resolution ) # Re-arrange event study data to be in long format (temporarily) if self . formatting == 'es' : self . data = self . data . rename ({ 'f1i' : 'fid' , 'y1' : 'comp' }, axis = 1 ) self . data = pd . concat ([ self . data , self . data [[ 'f2i' , 'y2' ]] . rename ({ 'f2i' : 'fid' , 'y2' : 'comp' }, axis = 1 )], axis = 0 ) if grouping == 'quantile_all' : # Get quantiles from all data quantile_groups = self . data [ 'comp' ] . quantile ( quantiles ) # Generate firm-level cdfs for i , quant in enumerate ( quantile_groups ): cdfs [:, i ] = self . data . assign ( firm_quant = lambda d : d [ 'comp' ] <= quant ) . groupby ( 'fid' )[ 'firm_quant' ] . agg ( sum ) . to_numpy () # Normalize by firm size (convert to cdf) fsize = self . data . groupby ( 'fid' ) . size () . to_numpy () cdfs /= np . expand_dims ( fsize , 1 ) elif grouping in [ 'quantile_firm_small' , 'quantile_firm_large' ]: # Sort data by compensation (do this once now, so that don't need to do it again later) (also note it is faster to sort then manually compute quantiles than to use built-in quantile functions) self . data = self . data . sort_values ([ 'comp' ]) if grouping == 'quantile_firm_small' : # Convert pandas dataframe into a dictionary to access data faster # Source for idea: https://stackoverflow.com/questions/57208997/looking-for-the-fastest-way-to-slice-a-row-in-a-huge-pandas-dataframe # Source for how to actually format data correctly: https://stackoverflow.com/questions/56064677/pandas-series-to-dict-with-repeated-indices-make-dict-with-list-values data_dict = self . data [ 'comp' ] . groupby ( level = 0 ) . agg ( list ) . to_dict () # Generate the cdfs for row in tqdm ( range ( n_firms )): fid = row + 1 # fids start at 1 # Get the firm-level compensation data (don't need to sort because already sorted) if grouping == 'quantile_firm_small' : comp = data_dict [ fid ] elif grouping == 'quantile_firm_large' : comp = self . data . loc [ self . data [ 'fid' ] == fid , 'comp' ] # Generate the firm-level cdf # Note: update numpy array element by element # Source: https://stackoverflow.com/questions/30012362/faster-way-to-convert-list-of-objects-to-numpy-array/30012403 for i in range ( cdf_resolution ): index = max ( len ( comp ) * ( i + 1 ) // cdf_resolution - 1 , 0 ) # Don't want negative index # Update cdfs with the firm-level cdf cdfs [ row , i ] = comp [ index ] # Drop rows that were appended earlier and rename columns if self . formatting == 'es' : self . data = self . data . dropna () self . data = self . data . rename ({ 'fid' : 'f1i' , 'comp' : 'y1' }, axis = 1 ) return cdfs","title":"approx_cdfs()"},{"location":"twfe_network-reference/#twfe_network.twfe_network.cluster","text":"!!! purpose Cluster data and assign a new column giving the cluster for each firm Parameters: Name Type Description Default cdf_resolution int how many values to use to approximate the cdf 10 grouping string how to group the cdfs ('quantile_all' to get quantiles from entire set of data, then have firm-level values between 0 and 1; 'quantile_firm_small' to get quantiles at the firm-level and have values be compensations if small data; 'quantile_firm_large' to get quantiles at the firm-level and have values be compensations if large data, note that this is up to 50 times slower than 'quantile_firm_small' and should only be used if the dataset is too large to copy into a dictionary) 'quantile_all' year int if None, uses entire dataset; if int, gives year of data to consider None user_KMeans dict use parameters defined in KMeans_dict for KMeans estimation (for more information on what parameters can be used, visit https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), and use default parameters defined in class attribute default_KMeans for any parameters not specified {} Returns: Type Description Nothing Source code in pytwoway/twfe_network.py def cluster ( self , cdf_resolution = 10 , grouping = 'quantile_all' , year = None , user_KMeans = {}): ''' Purpose: Cluster data and assign a new column giving the cluster for each firm Arguments: cdf_resolution (int): how many values to use to approximate the cdf grouping (string): how to group the cdfs ('quantile_all' to get quantiles from entire set of data, then have firm-level values between 0 and 1; 'quantile_firm_small' to get quantiles at the firm-level and have values be compensations if small data; 'quantile_firm_large' to get quantiles at the firm-level and have values be compensations if large data, note that this is up to 50 times slower than 'quantile_firm_small' and should only be used if the dataset is too large to copy into a dictionary) year (int): if None, uses entire dataset; if int, gives year of data to consider user_KMeans (dict): use parameters defined in KMeans_dict for KMeans estimation (for more information on what parameters can be used, visit https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), and use default parameters defined in class attribute default_KMeans for any parameters not specified Returns: Nothing ''' if self . formatting == 'es' : # Compute cdfs cdfs = self . approx_cdfs ( cdf_resolution = cdf_resolution , grouping = grouping , year = year ) logger . info ( 'firm cdfs computed' ) # Compute firm clusters KMeans_params = self . update_dict ( self . default_KMeans , user_KMeans ) clusters = KMeans ( n_clusters = KMeans_params [ 'n_clusters' ], init = KMeans_params [ 'init' ], n_init = KMeans_params [ 'n_init' ], max_iter = KMeans_params [ 'max_iter' ], tol = KMeans_params [ 'tol' ], precompute_distances = KMeans_params [ 'precompute_distances' ], verbose = KMeans_params [ 'verbose' ], random_state = KMeans_params [ 'random_state' ], copy_x = KMeans_params [ 'copy_x' ], n_jobs = KMeans_params [ 'n_jobs' ], algorithm = KMeans_params [ 'algorithm' ]) . fit ( cdfs ) . labels_ + 1 # Need +1 because need > 0 logger . info ( 'firm clusters computed' ) # Create Pandas dataframe linking fid to firm cluster n_firms = cdfs . shape [ 0 ] fids = np . linspace ( 0 , n_firms - 1 , n_firms ) + 1 clusters_dict_1 = { 'f1i' : fids , 'j1' : clusters } clusters_dict_2 = { 'f2i' : fids , 'j2' : clusters } clusters_df_1 = pd . DataFrame ( clusters_dict_1 , index = fids ) clusters_df_2 = pd . DataFrame ( clusters_dict_2 , index = fids ) logger . info ( 'dataframes linked fids to clusters generated' ) # Merge into event study data self . data = self . data . merge ( clusters_df_1 , how = 'left' , on = 'f1i' ) self . data = self . data . merge ( clusters_df_2 , how = 'left' , on = 'f2i' ) logger . info ( 'clusters merged into event study data' ) # Correct datatypes self . data [[ 'f1i' , 'f2i' , 'm' ]] = self . data [[ 'f1i' , 'f2i' , 'm' ]] . astype ( int ) logger . info ( 'datatypes of clusters corrected' )","title":"cluster()"},{"location":"twfe_network-reference/#twfe_network.twfe_network.conset","text":"!!! purpose Update data to include only the largest connected set of movers, and if firm ids are contiguous, also return the NetworkX Graph Returns: Type Description G (NetworkX Graph) largest connected set of movers (only returns if firm ids are contiguous, otherwise returns None) Source code in pytwoway/twfe_network.py def conset ( self ): ''' Purpose: Update data to include only the largest connected set of movers, and if firm ids are contiguous, also return the NetworkX Graph Arguments: Nothing Returns: G (NetworkX Graph): largest connected set of movers (only returns if firm ids are contiguous, otherwise returns None) ''' prev_workers = self . n_workers () if self . formatting == 'long' : # Add max firm id per worker to serve as a central node for the worker # self.data['fid_f1'] = self.data.groupby('wid')['fid'].transform(lambda a: a.shift(-1)) # FIXME - this is directed but is much slower self . data [ 'fid_max' ] = self . data . groupby ([ 'wid' ])[ 'fid' ] . transform ( max ) # FIXME - this is undirected but is much faster # Find largest connected set # Source: https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.components.connected_components.html G = nx . from_pandas_edgelist ( self . data , 'fid' , 'fid_max' ) # Drop fid_max self . data = self . data . drop ([ 'fid_max' ], axis = 1 ) # Update data if not connected if not self . connected : largest_cc = max ( nx . connected_components ( G ), key = len ) # Keep largest connected set of firms self . data = self . data [ self . data [ 'fid' ] . isin ( largest_cc )] elif self . formatting == 'es' : # Find largest connected set # Source: https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.components.connected_components.html G = nx . from_pandas_edgelist ( self . data , 'f1i' , 'f2i' ) # Update data if not connected if not self . connected : largest_cc = max ( nx . connected_components ( G ), key = len ) # Keep largest connected set of firms self . data = self . data [( self . data [ 'f1i' ] . isin ( largest_cc )) & ( self . data [ 'f2i' ] . isin ( largest_cc ))] # Data is now connected self . connected = True # If connected data != full data, set contiguous to False if prev_workers != self . n_workers (): self . contiguous = False # Return G if firm ids are contiguous (if they're not contiguous, they have to be updated first) if self . contiguous : return G return None","title":"conset()"},{"location":"twfe_network-reference/#twfe_network.twfe_network.contiguous_fids","text":"!!! purpose Make firm ids contiguous Returns: Type Description Nothing Source code in pytwoway/twfe_network.py def contiguous_fids ( self ): ''' Purpose: Make firm ids contiguous Arguments: Nothing Returns: Nothing ''' # Generate fid_list (note that all columns listed in fid_list are included in the set of firm ids, and all columns are adjusted to have the new, contiguous firm ids) if self . formatting == 'long' : fid_list = [ 'fid' ] elif self . formatting == 'es' : fid_list = [ 'f1i' , 'f2i' ] # Create sorted set of unique fids fids = [] for fid in fid_list : fids += list ( self . data [ fid ] . unique ()) fids = sorted ( list ( set ( fids ))) # Create list of adjusted fids adjusted_fids = np . linspace ( 0 , len ( fids ) - 1 , len ( fids )) . astype ( int ) + 1 # Update each fid one at a time for fid in fid_list : # Create dictionary linking current to new fids, then convert into a dataframe for merging fids_dict = { fid : fids , 'adj_' + fid : adjusted_fids } fids_df = pd . DataFrame ( fids_dict , index = adjusted_fids ) # Merge new, contiguous fids into event study data self . data = self . data . merge ( fids_df , how = 'left' , on = fid ) # Drop old fid column and rename contiguous fid column self . data = self . data . drop ([ fid ], axis = 1 ) self . data = self . data . rename ({ 'adj_' + fid : fid }, axis = 1 ) # Firm ids are now contiguous self . contiguous = True","title":"contiguous_fids()"},{"location":"twfe_network-reference/#twfe_network.twfe_network.data_validity","text":"!!! purpose Check that data is formatted correctly. Results are logged Returns: Type Description Nothing Source code in pytwoway/twfe_network.py def data_validity ( self ): ''' Purpose: Check that data is formatted correctly. Results are logged Arguments: Nothing Returns: Nothing ''' if self . formatting == 'long' : success = True logger . info ( '--- checking columns ---' ) cols = True for col in [ 'wid' , 'comp' , 'fid' , 'year' ]: if self . col_dict [ col ] not in self . data . columns : logger . info ( col , 'missing from data' ) cols = False else : if col == 'year' : if self . data [ self . col_dict [ col ]] . dtype != 'int' : logger . info ( self . col_dict [ col ], 'has wrong dtype, should be int but is' , self . data [ self . col_dict [ col ]] . dtype ) cols = False elif col == 'comp' : if self . data [ self . col_dict [ col ]] . dtype != 'float64' : logger . info ( self . col_dict [ col ], 'has wrong dtype, should be float64 but is' , self . data [ self . col_dict [ col ]] . dtype ) cols = False logger . info ( 'columns correct:' + str ( cols )) if not cols : success = False raise ValueError ( 'Your data does not include the correct columns. The twfe_network object cannot be generated with your data.' ) else : # Correct column names self . update_cols () logger . info ( '--- checking worker-year observations ---' ) max_obs = self . data . groupby ([ 'wid' , 'year' ]) . size () . max () logger . info ( 'max number of worker-year observations (should be 1):' + str ( max_obs )) if max_obs > 1 : success = False logger . info ( '--- checking nan data ---' ) nan = self . data . shape [ 0 ] - self . data . dropna () . shape [ 0 ] logger . info ( 'data nan rows (should be 0):' + str ( nan )) if nan > 0 : success = False logger . info ( '--- checking connected set ---' ) self . data [ 'fid_max' ] = self . data . groupby ([ 'wid' ])[ 'fid' ] . transform ( max ) G = nx . from_pandas_edgelist ( self . data , 'fid' , 'fid_max' ) largest_cc = max ( nx . connected_components ( G ), key = len ) self . data = self . data . drop ([ 'fid_max' ], axis = 1 ) outside_cc = self . data [( ~ self . data [ 'fid' ] . isin ( largest_cc ))] . shape [ 0 ] logger . info ( 'observations outside connected set (should be 0):' + str ( outside_cc )) if outside_cc > 0 : success = False logger . info ( 'Overall success:' + str ( success )) elif self . formatting == 'es' : success_stayers = True success_movers = True logger . info ( '--- checking columns ---' ) cols = True for col in [ 'wid' , 'y1' , 'y2' , 'f1i' , 'f2i' , 'm' ]: if self . col_dict [ col ] not in self . data . columns : logger . info ( col , 'missing from stayers' ) cols = False else : if col in [ 'y1' , 'y2' ]: if self . data [ self . col_dict [ col ]] . dtype != 'float64' : logger . info ( col , 'has wrong dtype, should be float64 but is' , self . data [ self . col_dict [ col ]] . dtype ) cols = False elif col == 'm' : if self . data [ self . col_dict [ col ]] . dtype != 'int' : logger . info ( col , 'has wrong dtype, should be int but is' , self . data [ self . col_dict [ col ]] . dtype ) cols = False logger . info ( 'columns correct:' + str ( cols )) if not cols : success_stayers = False success_movers = False raise ValueError ( 'Your data does not include the correct columns. The twfe_network object cannot be generated with your data.' ) else : # Correct column names self . update_cols () stayers = self . data [ self . data [ 'm' ] == 0 ] movers = self . data [ self . data [ 'm' ] == 1 ] logger . info ( '--- checking rows ---' ) na_stayers = stayers . shape [ 0 ] - stayers . dropna () . shape [ 0 ] na_movers = movers . shape [ 0 ] - movers . dropna () . shape [ 0 ] logger . info ( 'stayers nan rows (should be 0):' + str ( na_stayers )) logger . info ( 'movers nan rows (should be 0):' + str ( na_movers )) if na_stayers > 0 : success_stayers = False if na_movers > 0 : success_movers = False logger . info ( '--- checking firms ---' ) firms_stayers = ( stayers [ 'f1i' ] != stayers [ 'f2i' ]) . sum () firms_movers = ( movers [ 'f1i' ] == movers [ 'f2i' ]) . sum () logger . info ( 'stayers with different firms (should be 0):' + str ( firms_stayers )) logger . info ( 'movers with same firm (should be 0):' + str ( firms_movers )) if firms_stayers > 0 : success_stayers = False if firms_movers > 0 : success_movers = False logger . info ( '--- checking income ---' ) income_stayers = ( stayers [ 'y1' ] != stayers [ 'y2' ]) . sum () logger . info ( 'stayers with different income (should be 0):' + str ( income_stayers )) if income_stayers > 0 : success_stayers = False logger . info ( '--- checking connected set ---' ) G = nx . from_pandas_edgelist ( movers , 'f1i' , 'f2i' ) largest_cc = max ( nx . connected_components ( G ), key = len ) cc_stayers = stayers [( ~ stayers [ 'f1i' ] . isin ( largest_cc )) | ( ~ stayers [ 'f2i' ] . isin ( largest_cc ))] . shape [ 0 ] cc_movers = movers [( ~ movers [ 'f1i' ] . isin ( largest_cc )) | ( ~ movers [ 'f2i' ] . isin ( largest_cc ))] . shape [ 0 ] logger . info ( 'stayers outside connected set (should be 0):' + str ( cc_stayers )) logger . info ( 'movers outside connected set (should be 0):' + str ( cc_movers )) if cc_stayers > 0 : success_stayers = False if cc_movers > 0 : success_movers = False logger . info ( 'Overall success for stayers:' + str ( success_stayers )) logger . info ( 'Overall success for movers:' + str ( success_movers ))","title":"data_validity()"},{"location":"twfe_network-reference/#twfe_network.twfe_network.n_firms","text":"!!! purpose Get the number of unique firms Returns: Type Description (int) number of unique firms Source code in pytwoway/twfe_network.py def n_firms ( self ): ''' Purpose: Get the number of unique firms Arguments: Nothing Returns: (int): number of unique firms ''' if self . formatting == 'long' : return len ( self . data [ 'fid' ] . unique ()) elif self . formatting == 'es' : return len ( set ( list ( self . data [ 'f1i' ] . unique ()) + list ( self . data [ 'f2i' ] . unique ())))","title":"n_firms()"},{"location":"twfe_network-reference/#twfe_network.twfe_network.n_workers","text":"!!! purpose Get the number of unique workers Returns: Type Description (int) number of unique workers Source code in pytwoway/twfe_network.py def n_workers ( self ): ''' Purpose: Get the number of unique workers Arguments: Nothing Returns: (int): number of unique workers ''' return len ( self . data [ 'wid' ] . unique ())","title":"n_workers()"},{"location":"twfe_network-reference/#twfe_network.twfe_network.refactor_es","text":"!!! purpose Refactor long form data into event study data Returns: Type Description Nothing Source code in pytwoway/twfe_network.py def refactor_es ( self ): ''' Purpose: Refactor long form data into event study data Arguments: Nothing Returns: Nothing ''' if self . formatting == 'long' : # Sort data by wid and year self . data = self . data . sort_values ([ 'wid' , 'year' ]) logger . info ( 'data sorted by wid and year' ) # Introduce lagged fid and wid self . data [ 'fid_l1' ] = self . data [ 'fid' ] . shift ( periods = 1 ) self . data [ 'wid_l1' ] = self . data [ 'wid' ] . shift ( periods = 1 ) logger . info ( 'lagged fid introduced' ) # Generate spell ids # Source: https://stackoverflow.com/questions/59778744/pandas-grouping-and-aggregating-consecutive-rows-with-same-value-in-column new_spell = ( self . data [ 'fid' ] != self . data [ 'fid_l1' ]) | ( self . data [ 'wid' ] != self . data [ 'wid_l1' ]) # Allow for wid != wid_l1 to ensure that consecutive workers at the same firm get counted as different spells self . data [ 'spell_id' ] = new_spell . cumsum () logger . info ( 'spell ids generated' ) # Aggregate at the spell level spell = self . data . groupby ([ 'spell_id' ]) data_spell = spell . agg ( fid = pd . NamedAgg ( column = 'fid' , aggfunc = 'first' ), wid = pd . NamedAgg ( column = 'wid' , aggfunc = 'first' ), comp = pd . NamedAgg ( column = 'comp' , aggfunc = 'mean' ), year_start = pd . NamedAgg ( column = 'year' , aggfunc = 'min' ), year_end = pd . NamedAgg ( column = 'fid' , aggfunc = 'max' ) ) logger . info ( 'data aggregated at the spell level' ) ## Format as event study ## # Split workers by spell count spell_count = data_spell . groupby ([ 'wid' ]) . size () single_spell = spell_count [ spell_count == 1 ] . index stayers = data_spell [ data_spell [ 'wid' ] . isin ( single_spell )] mult_spell = spell_count [ spell_count > 1 ] . index movers = data_spell [ data_spell [ 'wid' ] . isin ( mult_spell )] logger . info ( 'workers split by spell count' ) # Add lagged values movers = movers . sort_values ([ 'wid' , 'year_start' ]) movers [ 'fid_l1' ] = movers [ 'fid' ] . shift ( periods = 1 ) movers [ 'wid_l1' ] = movers [ 'wid' ] . shift ( periods = 1 ) movers [ 'comp_l1' ] = movers [ 'comp' ] . shift ( periods = 1 ) movers = movers [ movers [ 'wid' ] == movers [ 'wid_l1' ]] # Update columns stayers = stayers . rename ({ 'fid' : 'f1i' , 'comp' : 'y1' }, axis = 1 ) stayers [ 'f2i' ] = stayers [ 'f1i' ] stayers [ 'y2' ] = stayers [ 'y1' ] stayers [ 'm' ] = 0 movers = movers . rename ({ 'fid_l1' : 'f1i' , 'fid' : 'f2i' , 'comp_l1' : 'y1' , 'comp' : 'y2' }, axis = 1 ) movers [ 'f1i' ] = movers [ 'f1i' ] . astype ( int ) movers [ 'm' ] = 1 # Keep only relevant columns stayers = stayers [[ 'wid' , 'y1' , 'y2' , 'f1i' , 'f2i' , 'm' ]] movers = movers [[ 'wid' , 'y1' , 'y2' , 'f1i' , 'f2i' , 'm' ]] logger . info ( 'columns updated' ) # Merge stayers and movers self . data = pd . concat ([ stayers , movers ]) # Update col_dict self . col_dict = { 'wid' : 'wid' , 'y1' : 'y1' , 'y2' : 'y2' , 'f1i' : 'f1i' , 'f2i' : 'f2i' , 'm' : 'm' } logger . info ( 'data reformatted as event study' ) # Data is now formatted as event study self . formatting = 'es'","title":"refactor_es()"},{"location":"twfe_network-reference/#twfe_network.twfe_network.run_akm_corrected","text":"!!! purpose Run bias-corrected AKM estimator Parameters: Name Type Description Default user_akm dictionary dictionary of parameters for bias-corrected AKM estimation Dictionary parameters: ncore (int): number of cores to use batch (int): batch size to send in parallel ndraw_pii (int): number of draw to use in approximation for leverages ndraw_tr (int): number of draws to use in approximation for traces check (bool): whether to compute the non-approximated estimates as well hetero (bool): whether to compute the heteroskedastic estimates out (string): outputfile con (string): computes the smallest eigen values, this is the filepath where these results are saved logfile (string): log output to a logfile levfile (string): file to load precomputed leverages statsonly (bool): save data statistics only {} Returns: Type Description akm_res (dictionary) dictionary of results Source code in pytwoway/twfe_network.py def run_akm_corrected ( self , user_akm = {}): ''' Purpose: Run bias-corrected AKM estimator Arguments: user_akm (dictionary): dictionary of parameters for bias-corrected AKM estimation Dictionary parameters: ncore (int): number of cores to use batch (int): batch size to send in parallel ndraw_pii (int): number of draw to use in approximation for leverages ndraw_tr (int): number of draws to use in approximation for traces check (bool): whether to compute the non-approximated estimates as well hetero (bool): whether to compute the heteroskedastic estimates out (string): outputfile con (string): computes the smallest eigen values, this is the filepath where these results are saved logfile (string): log output to a logfile levfile (string): file to load precomputed leverages statsonly (bool): save data statistics only Returns: akm_res (dictionary): dictionary of results ''' akm_params = self . update_dict ( self . default_akm , user_akm ) akm_params [ 'data' ] = self . data # Make sure to use up-to-date data akm_res = feacf . FEsolver ( akm_params ) . res # feacf.main(akm_params) # FIXME corrected for new feacf file using class structure return akm_res","title":"run_akm_corrected()"},{"location":"twfe_network-reference/#twfe_network.twfe_network.run_cre","text":"!!! purpose Run CRE estimator Parameters: Name Type Description Default user_cre dictionary dictionary of parameters for CRE estimation Dictionary parameters: ncore (int): number of cores to use ndraw_tr (int): number of draws to use in approximation for traces ndp (int): number of draw to use in approximation for leverages out (string): outputfile posterior (bool): compute posterior variance wobtw (bool): sets between variation to 0, pure RE {} Returns: Type Description cre_res (dictionary) dictionary of results Source code in pytwoway/twfe_network.py def run_cre ( self , user_cre = {}): ''' Purpose: Run CRE estimator Arguments: user_cre (dictionary): dictionary of parameters for CRE estimation Dictionary parameters: ncore (int): number of cores to use ndraw_tr (int): number of draws to use in approximation for traces ndp (int): number of draw to use in approximation for leverages out (string): outputfile posterior (bool): compute posterior variance wobtw (bool): sets between variation to 0, pure RE Returns: cre_res (dictionary): dictionary of results ''' cre_params = self . update_dict ( self . default_cre , user_cre ) cre_params [ 'data' ] = self . data # Make sure to use up-to-date data cre_res = cre . main ( cre_params ) return cre_res","title":"run_cre()"},{"location":"twfe_network-reference/#twfe_network.twfe_network.sim_network","text":"!!! purpose Simulate panel data corresponding to the calibrated model Parameters: Name Type Description Default params dictionary dictionary linking parameters to values Dictionary parameters: num_ind: number of workers num_time: time length of panel firm_size: max number of individuals per firm nk: number of firm types nl: number of worker types alpha_sig: standard error of individual fixed effect (volatility of worker effects) psi_sig: standard error of firm fixed effect (volatility of firm effects) w_sig: standard error of residual in AKM wage equation (volatility of wage shocks) csort: sorting effect cnetw: network effect csig: standard error of sorting/network effects p_move: probability a worker moves firms in any period required Returns: Type Description data (Pandas DataFrame) simulated network Source code in pytwoway/twfe_network.py def sim_network ( self , params ): ''' Purpose: Simulate panel data corresponding to the calibrated model Arguments: params (dictionary): dictionary linking parameters to values Dictionary parameters: num_ind: number of workers num_time: time length of panel firm_size: max number of individuals per firm nk: number of firm types nl: number of worker types alpha_sig: standard error of individual fixed effect (volatility of worker effects) psi_sig: standard error of firm fixed effect (volatility of firm effects) w_sig: standard error of residual in AKM wage equation (volatility of wage shocks) csort: sorting effect cnetw: network effect csig: standard error of sorting/network effects p_move: probability a worker moves firms in any period Returns: data (Pandas DataFrame): simulated network ''' # Generate fixed effects psi , alpha , G , H = self . sim_network_gen_fe ( params ) # Extract parameters num_ind , num_time , firm_size = params [ 'num_ind' ], params [ 'num_time' ], params [ 'firm_size' ] nk , nl , w_sig , p_move = params [ 'nk' ], params [ 'nl' ], params [ 'w_sig' ], params [ 'p_move' ] # Generate empty NumPy arrays network = np . zeros (( num_ind , num_time ), dtype = int ) spellcount = np . ones (( num_ind , num_time )) # Random draws of worker types for all individuals in panel sim_worker_types = randint ( low = 1 , high = nl , size = num_ind ) for i in range ( 0 , num_ind ): l = sim_worker_types [ i ] # At time 1, we draw from H for initial firm network [ i , 0 ] = choices ( range ( 0 , nk ), H [ l , :])[ 0 ] for t in range ( 1 , num_time ): # Hit moving shock if rand () < p_move : network [ i , t ] = choices ( range ( 0 , nk ), G [ l , network [ i , t - 1 ], :])[ 0 ] spellcount [ i , t ] = spellcount [ i , t - 1 ] + 1 else : network [ i , t ] = network [ i , t - 1 ] spellcount [ i , t ] = spellcount [ i , t - 1 ] # Compiling IDs and timestamps ids = np . reshape ( np . outer ( range ( 1 , num_ind + 1 ), np . ones ( num_time )), ( num_time * num_ind , 1 )) ids = ids . astype ( int )[:, 0 ] ts = np . reshape ( repmat ( range ( 1 , num_time + 1 ), num_ind , 1 ), ( num_time * num_ind , 1 )) ts = ts . astype ( int )[:, 0 ] # Compiling worker types types = np . reshape ( np . outer ( sim_worker_types , np . ones ( num_time )), ( num_time * num_ind , 1 )) alpha_data = alpha [ types . astype ( int )][:, 0 ] # Compiling firm types psi_data = psi [ np . reshape ( network , ( num_time * num_ind , 1 ))][:, 0 ] k_data = np . reshape ( network , ( num_time * num_ind , 1 ))[:, 0 ] # Compiling spell data spell_data = np . reshape ( spellcount , ( num_time * num_ind , 1 ))[:, 0 ] # Merging all columns into a dataframe data = pd . DataFrame ( data = { 'wid' : ids , 'year' : ts , 'k' : k_data , 'alpha' : alpha_data , 'psi' : psi_data , 'spell' : spell_data . astype ( int )}) # Generate size of spells dspell = data . groupby ([ 'wid' , 'spell' , 'k' ]) . size () . to_frame ( name = 'freq' ) . reset_index () # Draw firm ids dspell [ 'fid' ] = dspell . groupby ([ 'k' ])[ 'freq' ] . transform ( self . sim_network_draw_fids , * [ num_time , firm_size ]) # Make firm ids contiguous (and have them start at 1) dspell [ 'fid' ] = dspell . groupby ([ 'k' , 'fid' ])[ 'freq' ] . ngroup () + 1 # Merge spells into panel data = data . merge ( dspell , on = [ 'wid' , 'spell' , 'k' ]) data [ 'move' ] = ( data [ 'fid' ] != data [ 'fid' ] . shift ( 1 )) & ( data [ 'wid' ] == data [ 'wid' ] . shift ( 1 )) # Compute wages through the AKM formula data [ 'comp' ] = data [ 'alpha' ] + data [ 'psi' ] + w_sig * norm . rvs ( size = num_ind * num_time ) return data","title":"sim_network()"},{"location":"twfe_network-reference/#twfe_network.twfe_network.sim_network_draw_fids","text":"!!! purpose Draw firm ids for individual, given data that is grouped by worker id, spell id, and firm type Parameters: Name Type Description Default freq NumPy array size of groups (groups by worker id, spell id, and firm type) required num_time int time length of panel required firm_size int max number of individuals per firm required Returns: Type Description (NumPy array) random firms for each group Source code in pytwoway/twfe_network.py def sim_network_draw_fids ( self , freq , num_time , firm_size ): ''' Purpose: Draw firm ids for individual, given data that is grouped by worker id, spell id, and firm type Arguments: freq (NumPy array): size of groups (groups by worker id, spell id, and firm type) num_time (int): time length of panel firm_size (int): max number of individuals per firm Returns: (NumPy array): random firms for each group ''' max_int = np . int ( np . maximum ( 1 , freq . sum () / ( firm_size * num_time ))) return np . array ( np . random . choice ( max_int , size = freq . count ()) + 1 )","title":"sim_network_draw_fids()"},{"location":"twfe_network-reference/#twfe_network.twfe_network.sim_network_gen_fe","text":"!!! purpose Generate fixed effects values for simulated panel data corresponding to the calibrated model Parameters: Name Type Description Default params dictionary dictionary linking parameters to values Dictionary parameters: num_ind: number of workers num_time: time length of panel firm_size: max number of individuals per firm nk: number of firm types nl: number of worker types alpha_sig: standard error of individual fixed effect (volatility of worker effects) psi_sig: standard error of firm fixed effect (volatility of firm effects) w_sig: standard error of residual in AKM wage equation (volatility of wage shocks) csort: sorting effect cnetw: network effect csig: standard error of sorting/network effects p_move: probability a worker moves firms in any period required Returns: Type Description psi (NumPy array) array of firm fixed effects alpha (NumPy array): array of individual fixed effects G (NumPy array): transition matrices H (NumPy array): stationary distribution Source code in pytwoway/twfe_network.py def sim_network_gen_fe ( self , params ): ''' Purpose: Generate fixed effects values for simulated panel data corresponding to the calibrated model Arguments: params (dictionary): dictionary linking parameters to values Dictionary parameters: num_ind: number of workers num_time: time length of panel firm_size: max number of individuals per firm nk: number of firm types nl: number of worker types alpha_sig: standard error of individual fixed effect (volatility of worker effects) psi_sig: standard error of firm fixed effect (volatility of firm effects) w_sig: standard error of residual in AKM wage equation (volatility of wage shocks) csort: sorting effect cnetw: network effect csig: standard error of sorting/network effects p_move: probability a worker moves firms in any period Returns: psi (NumPy array): array of firm fixed effects alpha (NumPy array): array of individual fixed effects G (NumPy array): transition matrices H (NumPy array): stationary distribution ''' # Extract parameters nk , nl , alpha_sig , psi_sig = params [ 'nk' ], params [ 'nl' ], params [ 'alpha_sig' ], params [ 'psi_sig' ] csort , cnetw , csig = params [ 'csort' ], params [ 'cnetw' ], params [ 'csig' ] # Draw fixed effects psi = norm . ppf ( np . linspace ( 1 , nk , nk ) / ( nk + 1 )) * psi_sig alpha = norm . ppf ( np . linspace ( 1 , nl , nl ) / ( nl + 1 )) * alpha_sig # Generate transition matrices G = norm . pdf (( psi [ ax , ax , :] - cnetw * psi [ ax , :, ax ] - csort * alpha [:, ax , ax ]) / csig ) G = np . divide ( G , G . sum ( axis = 2 )[:, :, ax ]) # Generate empty stationary distributions H = np . ones (( nl , nk )) / nl # Solve stationary distributions for l in range ( 0 , nl ): # Solve eigenvectors # Source: https://stackoverflow.com/questions/31791728/python-code-explanation-for-stationary-distribution-of-a-markov-chain S , U = eig ( G [ l , :, :] . T ) stationary = np . array ( U [:, np . where ( np . abs ( S - 1. ) < 1e-8 )[ 0 ][ 0 ]] . flat ) stationary = stationary / np . sum ( stationary ) H [ l , :] = stationary return psi , alpha , G , H","title":"sim_network_gen_fe()"},{"location":"twfe_network-reference/#twfe_network.twfe_network.update_cols","text":"!!! purpose Rename columns and keep only relevant columns Returns: Type Description Nothing Source code in pytwoway/twfe_network.py def update_cols ( self ): ''' Purpose: Rename columns and keep only relevant columns Arguments: Nothing Returns: Nothing ''' if isinstance ( self . col_dict , dict ): if self . formatting == 'long' : self . data = self . data . rename ({ self . col_dict [ 'wid' ]: 'wid' , self . col_dict [ 'comp' ]: 'comp' , self . col_dict [ 'fid' ]: 'fid' , self . col_dict [ 'year' ]: 'year' }, axis = 1 ) self . data = self . data [[ 'wid' , 'comp' , 'fid' , 'year' ]] self . col_dict = { 'wid' : 'wid' , 'comp' : 'comp' , 'fid' : 'fid' , 'year' : 'year' } elif self . formatting == 'es' : self . data = self . data . rename ({ self . col_dict [ 'wid' ]: 'wid' , self . col_dict [ 'y1' ]: 'y1' , self . col_dict [ 'y2' ]: 'y2' , self . col_dict [ 'f1i' ]: 'f1i' , self . col_dict [ 'f2i' ]: 'f2i' , self . col_dict [ 'm' ]: 'm' }, axis = 1 ) self . data = self . data [[ 'wid' , 'y1' , 'y2' , 'f1i' , 'f2i' , 'm' ]] self . col_dict = { 'wid' : 'wid' , 'y1' : 'y1' , 'y2' : 'y2' , 'f1i' : 'f1i' , 'f2i' : 'f2i' , 'm' : 'm' }","title":"update_cols()"},{"location":"twfe_network-reference/#twfe_network.twfe_network.update_dict","text":"!!! purpose Replace entries in default_params with values in user_params. This function allows user_params to include only a subset of the required parameters in the dictionary Parameters: Name Type Description Default default_params dict default parameter values required user_params dict user selected parameter values required Returns: Type Description params (dict) default_params updated with parameter values in user_params Source code in pytwoway/twfe_network.py def update_dict ( self , default_params , user_params ): ''' Purpose: Replace entries in default_params with values in user_params. This function allows user_params to include only a subset of the required parameters in the dictionary Arguments: default_params (dict): default parameter values user_params (dict): user selected parameter values Returns: params (dict): default_params updated with parameter values in user_params ''' params = default_params . copy () params . update ( user_params ) return params","title":"update_dict()"},{"location":"twfe_network-reference/#twfe_network.twfe_monte_carlo","text":"!!! purpose Run Monte Carlo simulations of twfe_network to see the distribution of the true vs. estimated variance of psi and covariance between psi and alpha Parameters: Name Type Description Default N int number of simulations 500 ncore int how many cores to use 1 Other parameters see twfe_monte_carlo_interior() required Returns: Type Description true_psi_var (NumPy array) true simulated sample variance of psi true_psi_alpha_cov (NumPy array): true simulated sample covariance of psi and alpha akm_psi_var (NumPy array): AKM estimate of variance of psi akm_psi_alpha_cov (NumPy array): AKM estimate of covariance of psi and alpha akm_corr_psi_var (NumPy array): bias-corrected AKM estimate of variance of psi akm_corr_psi_alpha_cov (NumPy array): bias-corrected AKM estimate of covariance of psi and alpha cre_psi_var (NumPy array): CRE estimate of variance of psi cre_psi_alpha_cov (NumPy array): CRE estimate of covariance of psi and alpha Source code in pytwoway/twfe_network.py def twfe_monte_carlo ( N = 500 , ncore = 1 , data_params = {}, akm_params = {}, cre_params = {}, cdf_resolution = 10 , grouping = 'quantile_all' , year = None , KMeans_params = {}): ''' Purpose: Run Monte Carlo simulations of twfe_network to see the distribution of the true vs. estimated variance of psi and covariance between psi and alpha Arguments: N (int): number of simulations ncore (int): how many cores to use Other parameters: see twfe_monte_carlo_interior() Returns: true_psi_var (NumPy array): true simulated sample variance of psi true_psi_alpha_cov (NumPy array): true simulated sample covariance of psi and alpha akm_psi_var (NumPy array): AKM estimate of variance of psi akm_psi_alpha_cov (NumPy array): AKM estimate of covariance of psi and alpha akm_corr_psi_var (NumPy array): bias-corrected AKM estimate of variance of psi akm_corr_psi_alpha_cov (NumPy array): bias-corrected AKM estimate of covariance of psi and alpha cre_psi_var (NumPy array): CRE estimate of variance of psi cre_psi_alpha_cov (NumPy array): CRE estimate of covariance of psi and alpha ''' # Initialize NumPy arrays to store results true_psi_var = np . zeros ( N ) true_psi_alpha_cov = np . zeros ( N ) akm_psi_var = np . zeros ( N ) akm_psi_alpha_cov = np . zeros ( N ) akm_corr_psi_var = np . zeros ( N ) akm_corr_psi_alpha_cov = np . zeros ( N ) cre_psi_var = np . zeros ( N ) cre_psi_alpha_cov = np . zeros ( N ) # Use multi-processing if ncore > 1 : V = [] # Simulate networks with Pool ( processes = ncore ) as pool : V = pool . starmap ( twfe_monte_carlo_interior , [[ data_params , akm_params , cre_params , cdf_resolution , grouping , year , KMeans_params ] for _ in range ( N )]) for i , res in enumerate ( V ): true_psi_var [ i ], true_psi_alpha_cov [ i ], akm_psi_var [ i ], akm_psi_alpha_cov [ i ], akm_corr_psi_var [ i ], akm_corr_psi_alpha_cov [ i ], cre_psi_var [ i ], cre_psi_alpha_cov [ i ] = res else : for i in range ( N ): # Simulate a network true_psi_var [ i ], true_psi_alpha_cov [ i ], akm_psi_var [ i ], akm_psi_alpha_cov [ i ], akm_corr_psi_var [ i ], akm_corr_psi_alpha_cov [ i ], cre_psi_var [ i ], cre_psi_alpha_cov [ i ] = twfe_monte_carlo_interior ( data_params = data_params , akm_params = akm_params , cre_params = cre_params , cdf_resolution = cdf_resolution , grouping = grouping , year = year , KMeans_params = KMeans_params ) return true_psi_var , true_psi_alpha_cov , akm_psi_var , akm_psi_alpha_cov , akm_corr_psi_var , akm_corr_psi_alpha_cov , cre_psi_var , cre_psi_alpha_cov","title":"twfe_monte_carlo()"},{"location":"twfe_network-reference/#twfe_network.twfe_monte_carlo_interior","text":"!!! purpose Run Monte Carlo simulations of twfe_network to see the distribution of the true vs. estimated variance of psi and covariance between psi and alpha. This is the interior function to twfe_monte_carlo Parameters: Name Type Description Default data_params dictionary parameters for simulated data Dictionary parameters: num_ind: number of workers num_time: time length of panel firm_size: max number of individuals per firm nk: number of firm types nl: number of worker types alpha_sig: standard error of individual fixed effect (volatility of worker effects) psi_sig: standard error of firm fixed effect (volatility of firm effects) w_sig: standard error of residual in AKM wage equation (volatility of wage shocks) csort: sorting effect cnetw: network effect csig: standard error of sorting/network effects p_move: probability a worker moves firms in any period {} akm_params dictionary dictionary of parameters for bias-corrected AKM estimation Dictionary parameters: ncore (int): number of cores to use batch (int): batch size to send in parallel ndraw_pii (int): number of draw to use in approximation for leverages ndraw_tr (int): number of draws to use in approximation for traces check (bool): whether to compute the non-approximated estimates as well hetero (bool): whether to compute the heteroskedastic estimates out (string): outputfile con (string): computes the smallest eigen values, this is the filepath where these results are saved logfile (string): log output to a logfile levfile (string): file to load precomputed leverages statsonly (bool): save data statistics only {} cre_params dictionary dictionary of parameters for CRE estimation Dictionary parameters: ncore (int): number of cores to use ndraw_tr (int): number of draws to use in approximation for traces ndp (int): number of draw to use in approximation for leverages out (string): outputfile posterior (bool): compute posterior variance wobtw (bool): sets between variation to 0, pure RE {} Used for clustering cdf_resolution (int): how many values to use to approximate the cdf grouping (string): how to group the cdfs ('quantile_all' to get quantiles from entire set of data, then have firm-level values between 0 and 1; 'quantile_firm_small' to get quantiles at the firm-level and have values be compensations if small data; 'quantile_firm_large' to get quantiles at the firm-level and have values be compensations if large data, note that this is up to 50 times slower than 'quantile_firm_small' and should only be used if the dataset is too large to copy into a dictionary) year (int): if None, uses entire dataset; if int, gives year of data to consider KMeans_params (dict): use parameters defined in KMeans_dict for KMeans estimation (for more information on what parameters can be used, visit https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), and use default parameters defined in class attribute default_KMeans for any parameters not specified required Returns: Type Description true_psi_var (float) true simulated sample variance of psi true_psi_alpha_cov (float): true simulated sample covariance of psi and alpha akm_psi_var (float): AKM estimate of variance of psi akm_psi_alpha_cov (float): AKM estimate of covariance of psi and alpha akm_corr_psi_var (float): bias-corrected AKM estimate of variance of psi akm_corr_psi_alpha_cov (float): bias-corrected AKM estimate of covariance of psi and alpha cre_psi_var (float): CRE estimate of variance of psi cre_psi_alpha_cov (float): CRE estimate of covariance of psi and alpha Source code in pytwoway/twfe_network.py def twfe_monte_carlo_interior ( data_params = {}, akm_params = {}, cre_params = {}, cdf_resolution = 10 , grouping = 'quantile_all' , year = None , KMeans_params = {}): ''' Purpose: Run Monte Carlo simulations of twfe_network to see the distribution of the true vs. estimated variance of psi and covariance between psi and alpha. This is the interior function to twfe_monte_carlo Arguments: data_params (dictionary): parameters for simulated data Dictionary parameters: num_ind: number of workers num_time: time length of panel firm_size: max number of individuals per firm nk: number of firm types nl: number of worker types alpha_sig: standard error of individual fixed effect (volatility of worker effects) psi_sig: standard error of firm fixed effect (volatility of firm effects) w_sig: standard error of residual in AKM wage equation (volatility of wage shocks) csort: sorting effect cnetw: network effect csig: standard error of sorting/network effects p_move: probability a worker moves firms in any period akm_params (dictionary): dictionary of parameters for bias-corrected AKM estimation Dictionary parameters: ncore (int): number of cores to use batch (int): batch size to send in parallel ndraw_pii (int): number of draw to use in approximation for leverages ndraw_tr (int): number of draws to use in approximation for traces check (bool): whether to compute the non-approximated estimates as well hetero (bool): whether to compute the heteroskedastic estimates out (string): outputfile con (string): computes the smallest eigen values, this is the filepath where these results are saved logfile (string): log output to a logfile levfile (string): file to load precomputed leverages statsonly (bool): save data statistics only cre_params (dictionary): dictionary of parameters for CRE estimation Dictionary parameters: ncore (int): number of cores to use ndraw_tr (int): number of draws to use in approximation for traces ndp (int): number of draw to use in approximation for leverages out (string): outputfile posterior (bool): compute posterior variance wobtw (bool): sets between variation to 0, pure RE Used for clustering: cdf_resolution (int): how many values to use to approximate the cdf grouping (string): how to group the cdfs ('quantile_all' to get quantiles from entire set of data, then have firm-level values between 0 and 1; 'quantile_firm_small' to get quantiles at the firm-level and have values be compensations if small data; 'quantile_firm_large' to get quantiles at the firm-level and have values be compensations if large data, note that this is up to 50 times slower than 'quantile_firm_small' and should only be used if the dataset is too large to copy into a dictionary) year (int): if None, uses entire dataset; if int, gives year of data to consider KMeans_params (dict): use parameters defined in KMeans_dict for KMeans estimation (for more information on what parameters can be used, visit https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), and use default parameters defined in class attribute default_KMeans for any parameters not specified Returns: true_psi_var (float): true simulated sample variance of psi true_psi_alpha_cov (float): true simulated sample covariance of psi and alpha akm_psi_var (float): AKM estimate of variance of psi akm_psi_alpha_cov (float): AKM estimate of covariance of psi and alpha akm_corr_psi_var (float): bias-corrected AKM estimate of variance of psi akm_corr_psi_alpha_cov (float): bias-corrected AKM estimate of covariance of psi and alpha cre_psi_var (float): CRE estimate of variance of psi cre_psi_alpha_cov (float): CRE estimate of covariance of psi and alpha ''' # Simulate network nw = twfe_network ( data = data_params ) # Compute true sample variance of psi and covariance of psi and alpha psi_var = np . var ( nw . data [ 'psi' ]) psi_alpha_cov = np . cov ( nw . data [ 'psi' ], nw . data [ 'alpha' ])[ 0 , 1 ] # Convert into event study nw . refactor_es () # Estimate AKM model akm_res = nw . run_akm_corrected ( user_akm = akm_params ) # Cluster for CRE model nw . cluster ( cdf_resolution = cdf_resolution , grouping = grouping , year = year , user_KMeans = KMeans_params ) # Estimate CRE model cre_res = nw . run_cre ( user_cre = cre_params ) return psi_var , psi_alpha_cov , \\ akm_res [ 'var_fe' ], akm_res [ 'cov_fe' ], \\ akm_res [ 'var_ho' ], akm_res [ 'cov_ho' ], \\ cre_res [ 'var_wt' ] + cre_res [ 'var_bw' ], cre_res [ 'cov_wt' ] + cre_res [ 'cov_bw' ]","title":"twfe_monte_carlo_interior()"}]}